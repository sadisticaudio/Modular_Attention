{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadisticaudio/Modular_Attention/blob/main/Modular_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc890254-9e89-437f-a9a3-b0bb3a263b80",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "dc890254-9e89-437f-a9a3-b0bb3a263b80"
      },
      "source": [
        "# Transformer Modular Addition Through A Signal Processing Lens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d735f71-987a-4827-9ba5-258455219f3c",
      "metadata": {
        "id": "5d735f71-987a-4827-9ba5-258455219f3c"
      },
      "source": [
        "this analysis is based on previous work by Neel Nanda et al, \"Progress Measures for Grokking via Mechanistic Interpretability\".\\\n",
        "in that work, a model learns to implement an algorithm for modular addition that generalizes to unseen data and exhibits grokking behaviour.\\\n",
        "the hypothesis here is that the model is not using multiplication of trig terms to perform a rotation to perform the task,\\\n",
        "but that symmetries required by the algorithm are formed in the attention layer that are responsible for the model's ability to generalize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d787cdf-b86b-4bba-99c2-6d137e65c044",
      "metadata": {
        "scrolled": true,
        "id": "3d787cdf-b86b-4bba-99c2-6d137e65c044"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "!git clone https://github.com/sadisticaudio/checkpaint.git checkpaint_clone\n",
        "if cwd + '/checkpaint_clone/template/src' not in sys.path: sys.path.append(cwd + '/checkpaint_clone/template/src')\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  !pip install \"ipywidgets==7.8.5\"\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "  !pip install transformer_lens pylinalg pythreejs\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import einops\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
        "\n",
        "import checkpaint\n",
        "from checkpaint.utils import *\n",
        "from checkpaint.c_hooks import *\n",
        "from checkpaint import line_plot as lp\n",
        "\n",
        "p, d_model, d_mlp, n_heads, d_head = 113, 128, 512, 4, 32\n",
        "### CPU IS BEST. IN COLAB FOR SURE. WE ARE NOT DOING ANYTHING HEAVY HERE AND TRANSFERRING TO/FROM GPU MAKES PLOTTING A BIT SLOWER ###\n",
        "### CHANGE IF YOU'D LIKE ###\n",
        "device = \"cpu\"\n",
        "### THIS IS FROM NEEL NANDA'S ORIGINAL RUN, ONLY THE FINAL CHECKPOINT SAVED IN THIS FILE TO SAVE COLAB DISK USAGE\n",
        "### THE ORIGINAL FILE WAS NAMED \"full_run_data.pth\"\n",
        "end_run_data = torch.load(cwd + '/checkpaint_clone/end_run_data.pth', map_location=torch.device(device))\n",
        "\n",
        "prange, pprange = torch.arange(p).to(device), torch.arange(p*p).to(device)\n",
        "is_train, is_test = get_old_indices(device=device)\n",
        "train_indices, test_indices = pprange[is_train], pprange[is_test]\n",
        "dataset, labels = get_data(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b881dd01-c746-423b-a672-4fc209222835",
      "metadata": {
        "id": "b881dd01-c746-423b-a672-4fc209222835"
      },
      "outputs": [],
      "source": [
        "def showVector(x,**kwargs):\n",
        "    x = [inputs_last(t) for t in x] if isinstance(x, list) else inputs_last(x)\n",
        "    lp.draw_vector(x, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8365f41f-66e0-4333-a9a5-d43aa6423294",
      "metadata": {
        "id": "8365f41f-66e0-4333-a9a5-d43aa6423294"
      },
      "outputs": [],
      "source": [
        "### CREATE A CACHE STARTING WITH THE WEIGHTS OF THE FINAL MODEL IN THERE\n",
        "cache = squeeze_cache(CacheDict(end_run_data[\"state_dicts\"][-1]))\n",
        "\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = 1, n_heads = 4, d_model = 128, d_head = 32, d_mlp = 512, act_fn = \"relu\", normalization_type=None,\n",
        "    d_vocab=p+1, d_vocab_out=p, n_ctx=3, init_weights=True, device=device, seed = 598,\n",
        ")\n",
        "model = HookedTransformer(cfg)\n",
        "hooked_state_dict = model.state_dict()\n",
        "\n",
        "for name, x in hooked_state_dict.items():\n",
        "    if not name in cache and \"b_\" in name: cache[name] = torch.zeros_like(x)\n",
        "    if name == \"unembed.W_U\": cache[name] = cache[name][:,:-1]\n",
        "    if \"mask\" in name: cache[name] = x\n",
        "    if name in cache and cache[name].shape != x.shape: cache[name] = cache[name].transpose(-2,-1)\n",
        "    if \"W_O\" in name: cache[name] = cache[name].reshape(x.shape)\n",
        "    if not name in cache and \"IGNORE\" in name: cache[name] = x\n",
        "\n",
        "model.load_state_dict(cache)\n",
        "### RUN THE MODEL, ADD THE LOGITS TO THE CACHE\n",
        "cache[\"hook_logits\"], hooked_cache = model.run_with_cache(dataset)\n",
        "### ADD ALL OF THE ACTIVATIONS OF THE FINAL MODEL (FULL DATASET) TO THE CACHE\n",
        "cache.update(hooked_cache)\n",
        "key_freqs = get_top_k_freqs(cache[\"W_E\"].transpose(-2,-1), 5, -1, sumlist=[0])[0].sort()[0]\n",
        "key_harmonics, key_subharmonics = get_harmonics(key_freqs), get_subharmonics(key_freqs)\n",
        "tprint(\"key_freqs\", key_freqs, \"key_harmonics\", key_harmonics, \"key_subharmonics\", key_subharmonics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a214427-166f-4b9d-b859-57f1ab7a25d1",
      "metadata": {
        "id": "8a214427-166f-4b9d-b859-57f1ab7a25d1"
      },
      "outputs": [],
      "source": [
        "print(\"train loss\", cross_entropy_high_precision(cache[\"hook_logits\"][train_indices], labels[train_indices]))\n",
        "print(\"test loss\", cross_entropy_high_precision(cache[\"hook_logits\"][test_indices], labels[test_indices]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0bff32-7b18-4ed1-91d3-ac71a6917812",
      "metadata": {
        "scrolled": true,
        "id": "cf0bff32-7b18-4ed1-91d3-ac71a6917812"
      },
      "outputs": [],
      "source": [
        "cache = squeeze_cache(cache)\n",
        "ex_list = dataset.clone()\n",
        "ex_list[:,2] = labels\n",
        "ex_list = torch.cat((ex_list, pprange.unsqueeze(1).to(ex_list.device)), 1)\n",
        "\n",
        "def get_idx_by_pos(pos):\n",
        "    exs = ex_list.tolist()\n",
        "    pos = [pos] if type(pos) is not list else pos\n",
        "    for i, ps in reversed(list(enumerate(pos))):\n",
        "        exs = sorted(exs, key=lambda x: x[0 if ps == 'a' else 1 if ps == 'b' else 2 if ps == 'c' else 3])\n",
        "    return torch.tensor(exs, dtype=torch.long, device=device)[:,3].squeeze()\n",
        "\n",
        "a_idx, b_idx, c_idx = get_idx_by_pos('a'), get_idx_by_pos('b'), get_idx_by_pos('c')\n",
        "ac_idx, bc_idx, cb_idx = get_idx_by_pos(['a','c']), get_idx_by_pos(['b','c']), get_idx_by_pos(['c','b'])\n",
        "\n",
        "def a_hook(x):\n",
        "    hook_x = cache[x] if isinstance(x, str) else x\n",
        "    if hook_x.size(0) == p*p: return inputs_last(hook_x)\n",
        "    else:\n",
        "        print(\"doing a_hook on an inputs_last activation, take a look\")\n",
        "        return torch.empty(0)\n",
        "\n",
        "def inv_idx(idx):\n",
        "    s = sorted(range(len(idx)), key=idx.__getitem__)\n",
        "    return torch.tensor(s, dtype=torch.long, device=device) if isinstance(idx, torch.Tensor) else s\n",
        "c_idx_inv = inv_idx(c_idx)\n",
        "\n",
        "def c_hook(x):\n",
        "    hook_x = cache[x] if isinstance(x, str) else x\n",
        "    if hook_x.size(0) == p*p: return inputs_last(hook_x[c_idx])\n",
        "    else:\n",
        "        print(\"doing c_hook on an inputs_last activation, take a look\")\n",
        "        return torch.empty(0)\n",
        "\n",
        "def c_hook_inv(hook_x):\n",
        "    hook_x = hook_x.flatten(-2,-1)\n",
        "    hook_x = torch.moveaxis(hook_x, (-1), (0))\n",
        "    return hook_x[c_idx_inv]\n",
        "\n",
        "def make_wave(freq, phase=0): return torch.cos(prange*freq*2*torch.pi/p + phase)\n",
        "\n",
        "def print_loss_splits(logits, name=\"\"):\n",
        "    print(name + \" loss\", cross_entropy_high_precision(logits, labels).item())\n",
        "    print(name + \" train loss\", cross_entropy_high_precision(logits[is_train], labels[is_train]).item())\n",
        "    print(name + \" test loss\", cross_entropy_high_precision(logits[is_test], labels[is_test]).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c348b38c-49d8-4288-adf5-1ccc9c71b073",
      "metadata": {
        "id": "c348b38c-49d8-4288-adf5-1ccc9c71b073"
      },
      "source": [
        "### Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68678881-6cf3-469f-95a5-a12afd70ca31",
      "metadata": {
        "id": "68678881-6cf3-469f-95a5-a12afd70ca31"
      },
      "source": [
        "the problem is defined as modular addition of the tokens, in the form of (a + b) % p (113) = c\\\n",
        "the tokens are integer indices (0-112) and when they are embedded, they index a seperate length p sinusoid for every residual stream dimension.\\\n",
        "through training, the spectra of these sinusoids becomes sparse, with most of the magnitude being attributed to a set of key frequencies.\\\n",
        "below are the embeddings looking at just one dimension of the residual stream, token position on the x-axis.  shape: [ d_model, pos ]\\\n",
        "use the slider to scroll through the residual stream dimensions or click the \"spacetime\" button to toggle between spatial and fourier modes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6980a6-3ae5-47b2-a427-018f93c757ab",
      "metadata": {
        "id": "ea6980a6-3ae5-47b2-a427-018f93c757ab"
      },
      "outputs": [],
      "source": [
        "showVector(torch.stack([sd[\"embed.W_E\"] for sd in end_run_data[\"state_dicts\"]])[...,:-1].squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb42bfc-600e-407a-9cd9-eea5a2eb58de",
      "metadata": {
        "id": "6bb42bfc-600e-407a-9cd9-eea5a2eb58de"
      },
      "source": [
        "the key here is that for each dimension in the model, the pos axis indexes to a spot on a sinusoidal periodic function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1d01a6f-95cd-46e8-b447-e2f1fa774699",
      "metadata": {
        "id": "d1d01a6f-95cd-46e8-b447-e2f1fa774699"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55479907-a46b-4467-b8de-23803a93165a",
      "metadata": {
        "id": "55479907-a46b-4467-b8de-23803a93165a"
      },
      "source": [
        "in the attention layer, the model combines sines and cosines for a and b into one single cosine wave,\\\n",
        "centered midway between a and b.\n",
        "\\\n",
        "all transformations prior to the attention output are linear.\\\n",
        "prior to this, positions a & b are accessing the same waves but indexing at phase locations a & b.\\\n",
        "there are more details on linear maps below, but for now, just know that any linear transformation preserves this structure.\\\n",
        "the structure of the waves changes when the a & b value vectors are weighted and summed into the final \"=\" token position.\\\n",
        "at this point, each wave in [cosa, sina, cosb, sinb] are combined into a single cosine wave for each answer \"c\".\\\n",
        "\\\n",
        "in the previous work the entire input dataset [p * p] was typically reshaped to [p,p] with axes ordered as [a,b] for analysis\\\n",
        "here the dataset is also reshaped to [p,p] but with inputs ordered as [c,a], which will reveal symmetries in the activations\\\n",
        "\\\n",
        "here are dataset examples in the form [a, b, c] for c = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013d4f70-5a17-4b36-9c88-3ef599273994",
      "metadata": {
        "id": "013d4f70-5a17-4b36-9c88-3ef599273994"
      },
      "outputs": [],
      "source": [
        "for i in range(5): tprint(\"c = 0: example\", i, ex_list[c_idx][i][:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f5f75e-34ab-4a90-9bc2-f70707f80ec9",
      "metadata": {
        "id": "c9f5f75e-34ab-4a90-9bc2-f70707f80ec9"
      },
      "source": [
        "notice that for modular addition, as a increases, b decreases.\\\n",
        "this has the implication that the sinusoidal waves of each example are being indexed in reverse order of eachother.\\\n",
        "the same applies to other values of c, but with an offset applied to the \"b\" token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177d3933-f8db-4ebf-adf2-db354c069028",
      "metadata": {
        "id": "177d3933-f8db-4ebf-adf2-db354c069028"
      },
      "outputs": [],
      "source": [
        "for i in range(10): tprint(\"c = 6: example\", i, ex_list[c_idx][p * 6 + i][:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb6f900-9a1f-4199-bc8b-fb930ee6c632",
      "metadata": {
        "id": "8cb6f900-9a1f-4199-bc8b-fb930ee6c632"
      },
      "source": [
        "notice that in example 4 above, a and b are equal, because 3 + 3 = 6.\\\n",
        "also notice that in examples 2 & 4, a & b are swapped, as are examples 1 & 5, 0 & 6, etc.\\\n",
        "this spot is found at c/2, for all values of \"c\".\\\n",
        "since the value vectors are just a linear combination of the embeddings and are different spots on the same wave,\\\n",
        "when summed, a new wave is formed for each \"c\", and it is a sinusoid made entirely of cosines centered at c/2.\\\n",
        "these cosines can have a negative polarity, but they are always an even, symmetric function about c/2.\\\n",
        "\\\n",
        "below is the \"=\" token position of the final residual stream prior to unembedding.\\\n",
        "if you scroll the the c axis, note the symmetry present at c/2. axes are [d_model,c,a]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11e4b65-d740-4823-a9a2-cd6ffe0f6ad6",
      "metadata": {
        "id": "f11e4b65-d740-4823-a9a2-cd6ffe0f6ad6"
      },
      "outputs": [],
      "source": [
        "showVector(inputs_last(cache[\"resid_mid\"][c_idx])[-1], start_play_axis=1, full_mode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5151ebee-9045-4c99-8612-0ef95234ca43",
      "metadata": {
        "id": "5151ebee-9045-4c99-8612-0ef95234ca43"
      },
      "outputs": [],
      "source": [
        "def construct_resid_post_model():\n",
        "    \"\"\"CONSTRUCT A MODEL OF THE FINAL RESIDUAL STREAM\"\"\"\n",
        "    names = [\"cos_wave\", \"pos a\", \"pos b\", \"pos c\"]\n",
        "    r_model = { n: torch.zeros_like(inputs_last(cache[\"resid_post\"])[-1,:len(key_freqs)+1])[...,None].repeat(1,1,1,p) for n in names }\n",
        "    p_diag = torch.eye(p).bool().to(device)\n",
        "    for c in range(p):\n",
        "        symmetry_points = [c//2, (c//2 + p//2 + 1) % p]\n",
        "        symmetry_points.append(c//2 + 1 if c % 2 == 1 else (c//2 + p//2) % p)\n",
        "        index = (p + c - prange[...,None]) % p\n",
        "\n",
        "        for f, freq in enumerate(list(key_freqs)):\n",
        "            r_model[\"cos_wave\"][f][c] = torch.cos((prange - c/2) * freq * 2 * torch.pi / p)\n",
        "            r_model[\"pos c\"][f][c,...,symmetry_points] = r_model[\"cos_wave\"][f][c,...,symmetry_points]\n",
        "            r_model[\"pos a\"][f][c][p_diag] = r_model[\"cos_wave\"][f][c][p_diag]\n",
        "            b_wave = torch.cos(((p + c - prange) % p - c/2) * freq * 2 * torch.pi / p)[...,None]\n",
        "            r_model[\"pos b\"][f][c].scatter_(1, index, b_wave)\n",
        "            for name in r_model: r_model[name][-1][c] += r_model[name][f][c]\n",
        "\n",
        "    sum_max = r_model[\"cos_wave\"][-1].abs().max()\n",
        "    for name in r_model: r_model[name][-1] /= sum_max\n",
        "\n",
        "    return list(r_model.values())\n",
        "\n",
        "r_model = construct_resid_post_model()\n",
        "\n",
        "def get_cos_points(indices):\n",
        "    c, a, b = indices[-3], indices[-2], (indices[-3] - indices[-2] + p) % p\n",
        "    return [a, b, c/2, c/2 + p/2], [\"a\", \"b\", \"c/2\", \"c/2 + p/2\"]\n",
        "def get_cos_message(indices):\n",
        "    c, a, b = indices[-3], indices[-2], (indices[-3] - indices[-2] + p) % p\n",
        "    return str(a) + \" + \" + str(b) + \" % p = \" + str(c)\n",
        "\n",
        "def get_custom_points(indices): return [indices[-2]/2, indices[-2]/2 + p/2], [\"c/2\", \"c/2 + p/2\"]\n",
        "def get_custom_message(indices): return \"c = \" + str(indices[-2]) + \", c/2 = \" + str(indices[-2]/2) + \", c/2 + p/2 = \" + str(indices[-2]/2 + p/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c8baaf-e167-4de4-974b-18cffa414437",
      "metadata": {
        "id": "d4c8baaf-e167-4de4-974b-18cffa414437"
      },
      "source": [
        "below is a handcrafted function of length p with a strange shape with little sinusoidal content (top).\\\n",
        "the middle is a reversed version of the same function and the bottom is their sum.\\\n",
        "to seperate them on the graph, a bias has been applied afterward to the top and bottom functions.\\\n",
        "if you scroll through axis 0, the reversed version of the function is shifted prior to the summation.\\\n",
        "notice that there are actually two symmetry points. one is at shift/2 and the other at shift/2 + p/2.\\\n",
        "this is the case for all activations in the model post-attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da1d3023-7857-45ac-82af-91211b1632e1",
      "metadata": {
        "id": "da1d3023-7857-45ac-82af-91211b1632e1"
      },
      "outputs": [],
      "source": [
        "showVector(get_reversed_shifted_waves(make_custom_wave(device)), get_points=get_custom_points, get_message=get_custom_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112fe20c-cd70-4907-bea6-4be03c1ebcc6",
      "metadata": {
        "id": "112fe20c-cd70-4907-bea6-4be03c1ebcc6"
      },
      "source": [
        "below is an illustration of the structure of the activations simplified to unit magnitude cosines of the key frequencies.\\\n",
        "the shape is [frequency,c,a,b]. frequency includes a normalized sum in the last row.\\\n",
        "notice that if you scroll the c axis, a and b remain symmetrically placed around c/2.\\\n",
        "scrolling fully through the a axis, a and b wrap around, maintaining symmetry with respect to both c/2 and c/2 + p/2 in a circular fashion.\\\n",
        "note: this graph is in \"full_mode\" the slider/play axis can be changed, might be slow if in Colab, works much smoother locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e56ab30a-0c5f-46da-8b52-931c9898f151",
      "metadata": {
        "id": "e56ab30a-0c5f-46da-8b52-931c9898f151"
      },
      "outputs": [],
      "source": [
        "showVector(r_model, start_play_axis=1, full_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dcda7b7-6cde-4366-b143-97740c0b9265",
      "metadata": {
        "id": "7dcda7b7-6cde-4366-b143-97740c0b9265"
      },
      "source": [
        "\n",
        "\n",
        "to get this done, the model forms cosine waves over the inputs centered at c/2\\\n",
        "there are a number (~5) of key frequencies (key_freqs) for which these waves form\\\n",
        "each residual stream dimension forms it's own composite wave made up of len(key_freqs) cosines with different amplitudes,/\n",
        "but they are all symmetric about c/2.\\\n",
        "given that cosine is an even function and a and b are both equidistant from c/2, the model forms two symmetry points\\\n",
        "one is at c/2 and the other is at c/2 + p/2, allowing the modular addition to wrap around the inputs in a periodic fashion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd25f50-176d-402c-9c8b-96c51047fc57",
      "metadata": {
        "id": "2dd25f50-176d-402c-9c8b-96c51047fc57"
      },
      "source": [
        "there is more later that shows pattern weights for a & b are always equal, with (1 - their sum) going to the \"=\"./\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14fb5dc4-63c1-4867-bad1-252f51f829df",
      "metadata": {
        "id": "14fb5dc4-63c1-4867-bad1-252f51f829df"
      },
      "source": [
        "## Transformations of Waves"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3090b7e8-6266-42d5-8715-a45a9f2e6231",
      "metadata": {
        "id": "3090b7e8-6266-42d5-8715-a45a9f2e6231"
      },
      "source": [
        "most of the computations done by the model are linear transformations.\\\n",
        "in this work and the previous work, we are analyzing activations of the entire dataset.\\\n",
        "it is important to understand how sinusoids indexed by token position are affected by linear transformations.\\\n",
        "\\\n",
        "a linear map, in this case, takes a number of different (but typically similar) waves and weights them.\\\n",
        "each individual weight can only do two operations. it can scale the input wave and/or negate the input wave.\\\n",
        "after the weights are applied all these scaled/negated waves are summed into a single output dimension.\\\n",
        "this process is done for every dimension of the output.\\\n",
        "\\\n",
        "this gives the model the ability to manipulate the shape of these waves and adjust the magnitude and phase of the spectra in certain ways.\\\n",
        "for instance, the model can boost a certain frequency by applying large, positive weights to the waves that include that frequency\\\n",
        "and are in-phase with eachother while negating the weights for waves that are out-of-phase.\\\n",
        "frequencies can be attenuated with this mechanism by negating waves that are in-phase with eachother such that they sum to a low absolute value.\\\n",
        "in this sense, a linear map can be seen as a set of multi-channel linear-phase filters, each with one coefficient that sum at their output.\\\n",
        "\\\n",
        "this is on full display in the MLP. the W_in weights with respect to each neuron effectively act as a narrowband filter.\\\n",
        "since each weight can only scale and/or negate, there is obvious learned coordination to filter these neurons by frequency.\\\n",
        "for each neuron, there is one input weight for each residual stream dimension. to filter selectively for a desired frequency and phase,\\\n",
        "the neuron assigns a weight that is a cross correlation measure between the desired wave and the content at that residual stream dimension.\\\n",
        "\\\n",
        "a claim in the previous work, was that the model was multiplying waves in the MLP.\\\n",
        "reality is a bit weirder...  there is, in fact, a bump in harmonics of the dominant frequency of each neuron post ReLU,\\\n",
        "but this a consequence of how discontinuities in the time/spatial domain affect the frequency domain.\\\n",
        "if you multiply 2 waves together, the result is a composite wave made up of the sum/difference frequencies.\\\n",
        "so for 2 waves (14Hz & 35Hz), their product will be the sum of 2 waves (49Hz & 21Hz).\\\n",
        "if the 2 waves are the same frequency (14Hz), the result will be the sum of 2 waves (28 Hz - sum, 0Hz - diff). the 0Hz wave is just a bias.\\\n",
        "\\\n",
        "if you have a composite wave made up of 14Hz & 35Hz, multiplying this times itself will yeild these frequencies (0, 21, 28, 43, 49).\\\n",
        "the 43Hz component is a consequence of the Shannon-Nyquist theorem, where frequencies above the Nyquist limit,\\\n",
        "which is half the sample rate (113/2 = 56.5), wrap back below the limit in a mirror image.  Since 70 (35+35) is 13.5 above Nyquist,\\\n",
        "the sampling process craetes an alias of the 70Hz wave at 43Hz, which is 56.5 - 13.5.\\\n",
        "the ReLU is performing a different operation to acquire sum and difference frequencies.\\\n",
        "if you start with sinusoidal content and induce a discontinuity somehow, like by zeroing out part of it or distorting it with a ReLU,\\\n",
        "an infinite series of harmonics plus sums and differences between all frequency components will creep into your spectrum."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02143d61-7d29-4735-898c-3634589d208d",
      "metadata": {
        "id": "02143d61-7d29-4735-898c-3634589d208d"
      },
      "source": [
        "below is a ReLU applied to a single 14Hz wave, starting fully above zero, falling into negative territory, progressively affected by the ReLU.\\\n",
        "notice that while scrolling, a series of harmonics (integer multiples of 14) that mirror around the spectrum grow steadily.\\\n",
        "also we have 14Hz + 35Hz and 14Hz * 35Hz graphs below.  - shape: [a,b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c449f1bc-b10e-4e49-87f2-6572ce106daf",
      "metadata": {
        "id": "c449f1bc-b10e-4e49-87f2-6572ce106daf"
      },
      "outputs": [],
      "source": [
        "def relu_demo(x, **kwargs): showVector(F.relu(x - (x.max() - x.min()) * prange[...,None]/p - x.min()), **kwargs)\n",
        "\n",
        "wave14, wave35 = make_wave(14), make_wave(35)\n",
        "relu_demo(wave14, start_gui_type=\"fourier\")\n",
        "relu_demo(wave14 + wave35, start_gui_type=\"fourier\")\n",
        "relu_demo(wave14 * wave35, start_gui_type=\"fourier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d1f6663-c17a-489d-aceb-56041e8410f8",
      "metadata": {
        "id": "2d1f6663-c17a-489d-aceb-56041e8410f8"
      },
      "source": [
        "below the loss is computed from the neurons, starting with the unaltered neurons (\"hook_post\").\\\n",
        "next, the sum and difference frequencies are eliminated from the neurons post-ReLU and the model generalizes better.\\\n",
        "last, all frequencies aside from the key_freqs are eliminated and the loss is even better..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1938b2d-917a-4e40-89f9-768cb5436885",
      "metadata": {
        "id": "e1938b2d-917a-4e40-89f9-768cb5436885"
      },
      "outputs": [],
      "source": [
        "sums_and_diffs = get_sum_and_difference_frequencies(key_freqs)\n",
        "tprint(\"sums_and_diffs\", sums_and_diffs, \"length =\", len(sums_and_diffs))\n",
        "\n",
        "def remove_freqs_from_hook(hook_x, freqs):\n",
        "    HOOK_X = torch.fft.fft(inputs_last(hook_x))\n",
        "    HOOK_X[...,freqs] = HOOK_X[..., p - freqs] = 0\n",
        "    return inputs_first(torch.fft.ifft(HOOK_X).real)\n",
        "\n",
        "def test_neurons(hook_post, name):\n",
        "    mlp_out = einops.einsum(hook_post, cache[\"W_out\"], \"batch pos d_mlp, d_mlp d_model -> batch pos d_model\") + cache[\"b_out\"]\n",
        "    resid_post = cache[\"resid_mid\"] + mlp_out\n",
        "    logits = einops.einsum(resid_post, cache[\"W_U\"], \"batch pos d_model, d_model d_vocab -> batch pos d_vocab\")\n",
        "    print_loss_splits(logits, name)\n",
        "\n",
        "test_neurons(cache[\"post\"], \"regular\")\n",
        "test_neurons(remove_freqs_from_hook(cache[\"post\"], sums_and_diffs), \"without trig product terms\")\n",
        "test_neurons(remove_freqs_from_hook(cache[\"post\"], torch.tensor([i for i in range(1,p//2) if i not in key_freqs], device=device)), \"only key_freqs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be138baa-541b-4346-a67d-9e8acde181a9",
      "metadata": {
        "id": "be138baa-541b-4346-a67d-9e8acde181a9"
      },
      "outputs": [],
      "source": [
        "def get_top_freqs_and_indices(hook_x, dim=-1, *, sumlist=[-2], freqs_allowed=key_freqs):\n",
        "    freqs, mags = get_top_k_freqs(hook_x, 1, dim, sumlist=sumlist, freqs_allowed=freqs_allowed, squeeze=True)\n",
        "    idx_dict = {}\n",
        "    dim_range = torch.arange(freqs.numel(), device=freqs.device)\n",
        "    for freq in freqs.unique().tolist(): idx_dict[freq] = dim_range[freqs == freq]\n",
        "    freq_idx_sorted = freqs.sort()[1]\n",
        "    return freqs, mags, idx_dict, freq_idx_sorted\n",
        "\n",
        "neuron_freqs, neuron_mags, neuron_freq_idx, neuron_freq_idx_sorted = get_top_freqs_and_indices(inputs_last(cache[\"pre\"])[-1])\n",
        "\n",
        "test_freq = key_freqs[0].item()\n",
        "num_neur = len(neuron_freq_idx[test_freq])\n",
        "freq_pre = inputs_last(cache[\"pre\"])[-1,neuron_freq_idx[test_freq]]\n",
        "\n",
        "neuron_phases = torch.angle(torch.fft.fft(freq_pre[...,66,:])[...,test_freq])\n",
        "neuron_amps = torch.fft.fft(freq_pre[...,66,:])[...,test_freq].real\n",
        "neuron_phases_sorted, ordered_phase_idx = torch.sort(neuron_phases)#, descending=True)\n",
        "tprint(\"neuron_phases_sorted\", neuron_phases_sorted.shape, \"ordered_phase_idx\", ordered_phase_idx.shape, \"mags\", neuron_mags.shape, \"amps\", neuron_amps.shape)\n",
        "tprint(\"sorted phases, amps\", torch.cat((neuron_phases_sorted[...,None], neuron_amps[ordered_phase_idx][...,None]), -1))\n",
        "\n",
        "mock_freq_pre, mock_phases = torch.zeros([num_neur,p,p], device=device), torch.arange(num_neur, device=device) * 2 * torch.pi/num_neur - torch.pi\n",
        "wk = test_freq * 2 * torch.pi / p\n",
        "for d in range(num_neur):\n",
        "    mock_freq_pre[d] = torch.cos(mock_phases[d] - prange[...,None] * wk) + torch.cos(mock_phases[d] + prange[None] * wk)\n",
        "\n",
        "# mock_freq_pre = mock_freq_pre[inv_idx(ordered_phase_idx)]\n",
        "\n",
        "def running_sum_with_and_without_relu(pre, weights=cache[\"W_out\"][neuron_freq_idx[test_freq],0]):\n",
        "    non_relu_list = running_sum(pre,0, weights=weights, create_normalized_list=True)\n",
        "    post = F.relu(pre)\n",
        "    relu_list = running_sum(post,0, weights=weights, create_normalized_list=True)\n",
        "    max_mag = relu_list[0].abs().max()\n",
        "    return [relu_list[0] + max_mag, relu_list[1] + max_mag, non_relu_list[0] - max_mag, non_relu_list[1] - max_mag]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1b5dfe-bd20-4863-a15c-318e3ec5b152",
      "metadata": {
        "id": "4a1b5dfe-bd20-4863-a15c-318e3ec5b152"
      },
      "source": [
        "here we have the pre-activation for every neuron whose highest magnitude frequency is the first of the key_freqs.\\\n",
        "notice the movement of the waves as you scroll through the a axis.\\\n",
        "each neuron is approximately: cos$\\omega_{neur}$a+$\\phi_{neur}$ + cos$\\omega_{neur}$b+$\\phi_{neur}$  - shape: [neuron, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e3cbd2a-c1fd-4fa4-9ca5-0135c68e5a3b",
      "metadata": {
        "id": "7e3cbd2a-c1fd-4fa4-9ca5-0135c68e5a3b"
      },
      "outputs": [],
      "source": [
        "showVector(freq_pre[ordered_phase_idx], start_play_axis=0, full_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d8f3c2-1db0-459d-813b-217b022dc63f",
      "metadata": {
        "id": "d9d8f3c2-1db0-459d-813b-217b022dc63f"
      },
      "source": [
        "here we have a synthetic tensor made to reflect the neurons above.  it is an idealized set of unit 2D waves,\\\n",
        "equally spread across all phases ordered and spaced from -$\\pi$ to $\\pi$.  - shape: [neuron, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbfec2dd-13b0-436e-95a4-910d19a51434",
      "metadata": {
        "id": "cbfec2dd-13b0-436e-95a4-910d19a51434"
      },
      "outputs": [],
      "source": [
        "showVector(mock_freq_pre)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "209f98f7-1ae3-46da-bfea-f1c339031957",
      "metadata": {
        "id": "209f98f7-1ae3-46da-bfea-f1c339031957"
      },
      "source": [
        "below we use the handcrafted model above to demonstrate the affect of the ReLU on the neurons for this frequency.\\\n",
        "on the top of the graph are the ReLU activated mock neurons, the bottom are the mock neurons without the ReLU.\\\n",
        "the first axis scrolls through each mock neuron (red = pre-ReLU, magenta = post-ReLU).\\\n",
        "the cyan and orange are a running sum (cyan = pre-ReLU, orange = post-ReLU).\\\n",
        "since the plot starts at the last neuron index (43), cyan and orange are the sum of all neurons.\\\n",
        "notice that the ReLU cuts off the bottom of the pre-activated mock neuron (magenta),\\\n",
        "allowing the final a+b wave (cyan) to travel all the way from left to right as you index from 0 to 112.\\\n",
        "this is what gives the model the ability to dot with the same unembed weights from any a or b position.  - shape: [neuron, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd84e68-2a6b-4714-9c76-95126dbaf861",
      "metadata": {
        "id": "6fd84e68-2a6b-4714-9c76-95126dbaf861"
      },
      "outputs": [],
      "source": [
        "showVector(running_sum_with_and_without_relu(mock_freq_pre, weights=None), start_indices=[43], start_play_axis=1, full_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "110763c8-d63f-4e2a-9d07-27509e912305",
      "metadata": {
        "id": "110763c8-d63f-4e2a-9d07-27509e912305"
      },
      "source": [
        "below are the actual neuron activations for this frequency.\\\n",
        "the structure is similar, but there is noise that doen't perfectly cancel the orange wave (w/o ReLU) - shape: [neuron, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af08bee7-6702-449e-9341-9900dc591389",
      "metadata": {
        "id": "af08bee7-6702-449e-9341-9900dc591389"
      },
      "outputs": [],
      "source": [
        "showVector(running_sum_with_and_without_relu(freq_pre), start_indices=[43], start_play_axis=1, full_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e05c58-29d8-429e-8a3c-e635d1776ffe",
      "metadata": {
        "id": "20e05c58-29d8-429e-8a3c-e635d1776ffe"
      },
      "source": [
        "here we have the sum of all neurons associated with the test_freq after being mapped back to the residual stream.  - shape: [d_model, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c62ea2-b31c-442a-a4e1-56d225039d70",
      "metadata": {
        "id": "f2c62ea2-b31c-442a-a4e1-56d225039d70"
      },
      "outputs": [],
      "source": [
        "showVector(einops.einsum(F.relu(freq_pre),cache[\"W_out\"][neuron_freq_idx[test_freq]], \"neur posa posb, neur d_model -> d_model posa posb\") + cache[\"b_out\"][...,None,None], start_indices=[43], start_play_axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cb4b5fc-d042-4e27-9100-c9e9cfa7e493",
      "metadata": {
        "id": "2cb4b5fc-d042-4e27-9100-c9e9cfa7e493"
      },
      "source": [
        "next is the last position of the final residual stream with all frequencies zeroed except this one.\\\n",
        "notice the movement from right to left. this movement allows the examples to align with the answer \"c\" in W_U.  - shape: [d_model, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312818fa-ea4a-4da4-84d0-1711de4c34fa",
      "metadata": {
        "scrolled": true,
        "id": "312818fa-ea4a-4da4-84d0-1711de4c34fa"
      },
      "outputs": [],
      "source": [
        "showVector(pull_out_freqs(inputs_last(cache[\"resid_post\"])[-1], test_freq, dims=[-2,-1])[1], start_play_axis=-2, full_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a1853c4-2abb-41bd-b768-50f45d838994",
      "metadata": {
        "id": "5a1853c4-2abb-41bd-b768-50f45d838994"
      },
      "source": [
        "for reference, this that frequency pulled out before the MLP.  - shape: [d_model, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f585afac-6a66-4d0a-80b2-27b6d09c25cf",
      "metadata": {
        "id": "f585afac-6a66-4d0a-80b2-27b6d09c25cf"
      },
      "outputs": [],
      "source": [
        "showVector(pull_out_freqs(inputs_last(cache[\"resid_mid\"])[-1], test_freq, dims=[-2,-1])[1], start_play_axis=-2, full_mode=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) - jup",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}