{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadisticaudio/Modular_Attention/blob/main/Modular_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc890254-9e89-437f-a9a3-b0bb3a263b80",
      "metadata": {
        "id": "dc890254-9e89-437f-a9a3-b0bb3a263b80"
      },
      "source": [
        "# Transformer Modular Addition Through A Signal Processing Lens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d735f71-987a-4827-9ba5-258455219f3c",
      "metadata": {
        "id": "5d735f71-987a-4827-9ba5-258455219f3c"
      },
      "source": [
        "this analysis is based on previous work by Neel Nanda et al, \"Progress Measures for Grokking via Mechanistic Interpretability\".\\\n",
        "in that work, a model learns to implement an algorithm for modular addition that generalizes to unseen data and exhibits grokking behaviour.\\\n",
        "the hypothesis here is that the model is not using multiplication of trig terms to perform a rotation to perform the task,\\\n",
        "but that symmetries required by the algorithm are formed in the attention layer that are responsible for the model's ability to generalize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d787cdf-b86b-4bba-99c2-6d137e65c044",
      "metadata": {
        "scrolled": true,
        "id": "3d787cdf-b86b-4bba-99c2-6d137e65c044",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "!git clone https://github.com/sadisticaudio/checkpaint.git full_checkpaint\n",
        "!ls full_checkpaint/template/src\n",
        "if cwd + '/full_checkpaint/template/src' not in sys.path: sys.path.append(cwd + '/full_checkpaint/template/src')\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  # !pip install --upgrade ipywidgets\n",
        "  # !pip install \"ipywidgets>=7,<8\"\n",
        "  !pip install \"ipywidgets=7.8.5\"\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "  !pip install transformer_lens pylinalg pythreejs\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import einops\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
        "\n",
        "import checkpaint\n",
        "from checkpaint.utils import *\n",
        "from checkpaint.c_hooks import *\n",
        "from checkpaint import line_plot as lp\n",
        "\n",
        "model_paths = (cwd + '/full_checkpaint/end_run_data.pth', )#project_dir + '/full_run_data.pth',\n",
        "               # project_dir + '/grokking_demo_new.pth')\n",
        "\n",
        "model_num, p, d_model, d_mlp, n_heads, d_head = 0, 113, 128, 512, 4, 32\n",
        "print(\"model_paths\", model_paths, \"model_paths[model_num]\", model_paths[model_num])\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "full_run_data = torch.load(model_paths[model_num], map_location=torch.device(device))\n",
        "tprint(\"full_run_data keys\", full_run_data.keys())\n",
        "\n",
        "prange, pprange = torch.arange(p).to(device), torch.arange(p*p).to(device)\n",
        "is_train, is_test = get_old_indices(device=device) if model_num < 1 else get_new_indices(device=device)\n",
        "train_indices, test_indices = pprange[is_train], pprange[is_test]\n",
        "dataset, labels = get_data(device=device)\n",
        "\n",
        "printCudaMemUsage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fa9f111-ddbc-4f89-b1dd-6e70ac46c33c",
      "metadata": {
        "id": "3fa9f111-ddbc-4f89-b1dd-6e70ac46c33c"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip show ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b881dd01-c746-423b-a672-4fc209222835",
      "metadata": {
        "id": "b881dd01-c746-423b-a672-4fc209222835"
      },
      "outputs": [],
      "source": [
        "def showVector(x,**kwargs):\n",
        "    x = [inputs_last(t) for t in x] if isinstance(x, list) else inputs_last(x)\n",
        "    lp.draw_vector(x, **kwargs)\n",
        "\n",
        "showVector(torch.randn([12,23,p]), full_mode=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8365f41f-66e0-4333-a9a5-d43aa6423294",
      "metadata": {
        "id": "8365f41f-66e0-4333-a9a5-d43aa6423294"
      },
      "outputs": [],
      "source": [
        "cache = squeeze_cache(CacheDict(full_run_data[\"state_dicts\"][-1]))\n",
        "\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = 1, n_heads = 4, d_model = 128, d_head = 32, d_mlp = 512, act_fn = \"relu\", normalization_type=None,\n",
        "    d_vocab=p+1, d_vocab_out=p, n_ctx=3, init_weights=True, device=device, seed = 598,\n",
        ")\n",
        "model = HookedTransformer(cfg)\n",
        "hooked_state_dict = model.state_dict()\n",
        "\n",
        "for name, x in hooked_state_dict.items():\n",
        "    if not name in cache and \"b_\" in name: cache[name] = torch.zeros_like(x)\n",
        "    if name == \"unembed.W_U\": cache[name] = cache[name][:,:-1]\n",
        "    if \"mask\" in name: cache[name] = x\n",
        "    if name in cache and cache[name].shape != x.shape: cache[name] = cache[name].transpose(-2,-1)\n",
        "    if \"W_O\" in name: cache[name] = cache[name].reshape(x.shape)\n",
        "    if not name in cache and \"IGNORE\" in name: cache[name] = x\n",
        "\n",
        "model.load_state_dict(cache)\n",
        "cache[\"hook_logits\"], hooked_cache = model.run_with_cache(dataset)\n",
        "cache.update(hooked_cache)\n",
        "key_freqs = get_top_k_freqs(cache[\"W_E\"].transpose(-2,-1), 5, -1, sumlist=[0])[0].sort()[0]\n",
        "key_harmonics, key_subharmonics = get_second_harmonics(key_freqs), get_second_subharmonics(key_freqs)\n",
        "tprint(\"key_freqs\", key_freqs, \"key_harmonics\", key_harmonics, \"key_subharmonics\", key_subharmonics)\n",
        "printCudaMemUsage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a214427-166f-4b9d-b859-57f1ab7a25d1",
      "metadata": {
        "id": "8a214427-166f-4b9d-b859-57f1ab7a25d1"
      },
      "outputs": [],
      "source": [
        "print(\"train loss\", cross_entropy_high_precision(cache[\"hook_logits\"][train_indices], labels[train_indices]))\n",
        "print(\"test loss\", cross_entropy_high_precision(cache[\"hook_logits\"][test_indices], labels[test_indices]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0bff32-7b18-4ed1-91d3-ac71a6917812",
      "metadata": {
        "scrolled": true,
        "id": "cf0bff32-7b18-4ed1-91d3-ac71a6917812"
      },
      "outputs": [],
      "source": [
        "cache = squeeze_cache(cache)\n",
        "ex_list = dataset.clone()\n",
        "ex_list[:,2] = labels\n",
        "ex_list = torch.cat((ex_list, pprange.unsqueeze(1).to(ex_list.device)), 1)\n",
        "\n",
        "def get_idx_by_pos(pos):\n",
        "    exs = ex_list.tolist()\n",
        "    pos = [pos] if type(pos) is not list else pos\n",
        "    for i, ps in reversed(list(enumerate(pos))):\n",
        "        exs = sorted(exs, key=lambda x: x[0 if ps == 'a' else 1 if ps == 'b' else 2 if ps == 'c' else 3])\n",
        "    return torch.tensor(exs, dtype=torch.long, device=device)[:,3].squeeze()\n",
        "\n",
        "a_idx, b_idx, c_idx = get_idx_by_pos('a'), get_idx_by_pos('b'), get_idx_by_pos('c')\n",
        "ac_idx, bc_idx, cb_idx = get_idx_by_pos(['a','c']), get_idx_by_pos(['b','c']), get_idx_by_pos(['c','b'])\n",
        "\n",
        "def a_hook(x):\n",
        "    hook_x = cache[x] if isinstance(x, str) else x\n",
        "    if hook_x.size(0) == p*p: return inputs_last(hook_x)\n",
        "    else:\n",
        "        print(\"doing a_hook on an inputs_last activation, take a look\")\n",
        "        return torch.empty(0)\n",
        "\n",
        "def inv_idx(idx):\n",
        "    s = sorted(range(len(idx)), key=idx.__getitem__)\n",
        "    return torch.tensor(s, dtype=torch.long, device=device) if isinstance(idx, torch.Tensor) else s\n",
        "c_idx_inv = inv_idx(c_idx)\n",
        "\n",
        "def c_hook(x):\n",
        "    hook_x = cache[x] if isinstance(x, str) else x\n",
        "    if hook_x.size(0) == p*p: return inputs_last(hook_x[c_idx])\n",
        "    else:\n",
        "        print(\"doing c_hook on an inputs_last activation, take a look\")\n",
        "        return torch.empty(0)\n",
        "\n",
        "def c_hook_inv(hook_x):\n",
        "    hook_x = hook_x.flatten(-2,-1)\n",
        "    hook_x = torch.moveaxis(hook_x, (-1), (0))\n",
        "    return hook_x[c_idx_inv]\n",
        "\n",
        "printCudaMemUsage()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c348b38c-49d8-4288-adf5-1ccc9c71b073",
      "metadata": {
        "id": "c348b38c-49d8-4288-adf5-1ccc9c71b073"
      },
      "source": [
        "### Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68678881-6cf3-469f-95a5-a12afd70ca31",
      "metadata": {
        "id": "68678881-6cf3-469f-95a5-a12afd70ca31"
      },
      "source": [
        "the problem is defined as modular addition of the tokens, in the form of (a + b) % p (113) = c\\\n",
        "the tokens are integer indices (0-112) and when they are embedded, they index a seperate length p sinusoid for every residual stream dimension.\\\n",
        "through training, the spectra of these sinusoids becomes sparse, with most of the magnitude being attributed to a set of key frequencies.\\\n",
        "the axes in this plot are [ d_model, pos ]\\\n",
        "use the slider to scroll through the residual stream dimensions or click the \"spacetime\" button to toggle between spatial and fourier modes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6980a6-3ae5-47b2-a427-018f93c757ab",
      "metadata": {
        "scrolled": true,
        "id": "ea6980a6-3ae5-47b2-a427-018f93c757ab"
      },
      "outputs": [],
      "source": [
        "showVector(torch.stack([sd[\"embed.W_E\"] for sd in full_run_data[\"state_dicts\"]])[...,:-1].squeeze())\n",
        "all_embed_W_E = torch.stack([sd[\"embed.W_E\"] for sd in full_run_data[\"state_dicts\"]])[...,:-1]\n",
        "total_cache, total_model, total_hook = 0, 0, 0\n",
        "for name in cache.keys():\n",
        "    total_cache += cache[name].numel() * 4\n",
        "    if \"hook\" in name: total_hook += cache[name].numel() * 4\n",
        "    if \"hook\" not in name: total_model += cache[name].numel() * 4\n",
        "\n",
        "print(\"embed size\", to_human_readable(all_embed_W_E.numel() * 4))\n",
        "print(\"total size of cache\", to_human_readable(total_cache))\n",
        "print(\"total size of model\", to_human_readable(total_model))\n",
        "print(\"total size of hooks\", to_human_readable(total_hook))\n",
        "printCudaMemUsage()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb42bfc-600e-407a-9cd9-eea5a2eb58de",
      "metadata": {
        "id": "6bb42bfc-600e-407a-9cd9-eea5a2eb58de"
      },
      "source": [
        "the key here is that for each dimension in the model, the pos axis indexes to a spot on a sinusoidal periodic function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1d01a6f-95cd-46e8-b447-e2f1fa774699",
      "metadata": {
        "id": "d1d01a6f-95cd-46e8-b447-e2f1fa774699"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55479907-a46b-4467-b8de-23803a93165a",
      "metadata": {
        "id": "55479907-a46b-4467-b8de-23803a93165a"
      },
      "source": [
        "all transformations prior to the attention output are linear.\\\n",
        "prior to this, positions a & b are accessing the same waves but indexing phase locations linearly by a & b.\\\n",
        "there are more details on linear maps below, but for now, just know that any linear transformation preserves this structure.\\\n",
        "the structure of the waves changes when the a & b value vectors are weighted and summed into the final \"=\" token position.\\\n",
        "at this point, each wave in [cosa, sina, cosb, sinb] are combined into a single cosine wave for each answer \"c\".\\\n",
        "\\\n",
        "in the previous work the entire input dataset [p * p] was typically reshaped to [p,p] with axes ordered as [a,b] for analysis\\\n",
        "here the dataset is also reshaped to [p,p] but with inputs ordered as [c,a], which will reveal symmetries in the activations\\\n",
        "\\\n",
        "here are dataset examples in the form [a, b, c] for c = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013d4f70-5a17-4b36-9c88-3ef599273994",
      "metadata": {
        "id": "013d4f70-5a17-4b36-9c88-3ef599273994"
      },
      "outputs": [],
      "source": [
        "for i in range(5): tprint(\"c = 0: example\", i, ex_list[c_idx][i][:-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cbaXZTszE69Y"
      },
      "id": "cbaXZTszE69Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c9f5f75e-34ab-4a90-9bc2-f70707f80ec9",
      "metadata": {
        "id": "c9f5f75e-34ab-4a90-9bc2-f70707f80ec9"
      },
      "source": [
        "notice that for modular addition, as a increases, b decreases.\\\n",
        "this has the implication that the sinusoidal waves of each example are being indexed in reverse order of eachother.\\\n",
        "the same applies to other values of c, but with an offset applied to the \"b\" token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177d3933-f8db-4ebf-adf2-db354c069028",
      "metadata": {
        "id": "177d3933-f8db-4ebf-adf2-db354c069028"
      },
      "outputs": [],
      "source": [
        "for i in range(10): tprint(\"c = 8: example\", i, ex_list[c_idx][p * 8 + i][:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb6f900-9a1f-4199-bc8b-fb930ee6c632",
      "metadata": {
        "id": "8cb6f900-9a1f-4199-bc8b-fb930ee6c632"
      },
      "source": [
        "notice that in example 4 above, a and b are equal, because 4 + 4 = 8.\\\n",
        "also notice that in examples 3 & 5, a & b are swapped, as are examples 2 & 6, 1 & 7, etc.\\\n",
        "this spot is found at c/2, for all values of \"c\".\\\n",
        "since the value vectors are just a linear combination of the embeddings and are different spots on the same wave,\\\n",
        "when summed, a new wave is formed for each \"c\", and it is a sinusoid made entirely of cosines centered at c/2.\\\n",
        "these cosines can have a negative polarity, but they are always an even, symmetric function about c/2.\\\n",
        "\\\n",
        "below is the \"=\" token position of the final residual stream prior to unembedding.\\\n",
        "if you scroll the the c axis, note the symmetry present at c/2. axes are [d_model,c,a]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11e4b65-d740-4823-a9a2-cd6ffe0f6ad6",
      "metadata": {
        "id": "f11e4b65-d740-4823-a9a2-cd6ffe0f6ad6"
      },
      "outputs": [],
      "source": [
        "showVector(inputs_last(cache[\"resid_post\"][a_idx])[-1], start_play_axis=1, full_mode=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5151ebee-9045-4c99-8612-0ef95234ca43",
      "metadata": {
        "id": "5151ebee-9045-4c99-8612-0ef95234ca43"
      },
      "outputs": [],
      "source": [
        "def construct_resid_post_model():\n",
        "    \"\"\"CONSTRUCT A MODEL OF THE FINAL RESIDUAL STREAM\"\"\"\n",
        "    names = [\"cos_wave\", \"pos a\", \"pos b\", \"pos c\"]\n",
        "    r_model = { n: torch.zeros_like(inputs_last(cache[\"resid_post\"])[-1,:len(key_freqs)+1])[...,None].repeat(1,1,1,p) for n in names }\n",
        "    p_diag = torch.eye(p).bool().to(device)\n",
        "    for c in range(p):\n",
        "        symmetry_points = [c//2, (c//2 + p//2 + 1) % p]\n",
        "        symmetry_points.append(c//2 + 1 if c % 2 == 1 else (c//2 + p//2) % p)\n",
        "        index = (p + c - prange[...,None]) % p\n",
        "\n",
        "        for f, freq in enumerate(list(key_freqs)):\n",
        "            r_model[\"cos_wave\"][f][c] = torch.cos((prange - c/2) * freq * 2 * torch.pi / p)\n",
        "            r_model[\"pos c\"][f][c,...,symmetry_points] = r_model[\"cos_wave\"][f][c,...,symmetry_points]\n",
        "            r_model[\"pos a\"][f][c][p_diag] = r_model[\"cos_wave\"][f][c][p_diag]\n",
        "            b_wave = torch.cos(((p + c - prange) % p - c/2) * freq * 2 * torch.pi / p)[...,None]\n",
        "            r_model[\"pos b\"][f][c].scatter_(1, index, b_wave)\n",
        "            for name in r_model: r_model[name][-1][c] += r_model[name][f][c]\n",
        "\n",
        "    sum_max = r_model[\"cos_wave\"][-1].abs().max()\n",
        "    for name in r_model: r_model[name][-1] /= sum_max\n",
        "\n",
        "    return list(r_model.values())\n",
        "\n",
        "r_model = construct_resid_post_model()\n",
        "\n",
        "def get_cos_points(indices):\n",
        "    c, a, b = indices[-3], indices[-2], (indices[-3] - indices[-2] + p) % p\n",
        "    return [a, b, c/2, c/2 + p/2], [\"a\", \"b\", \"c/2\", \"c/2 + p/2\"]\n",
        "def get_cos_message(indices):\n",
        "    c, a, b = indices[-3], indices[-2], (indices[-3] - indices[-2] + p) % p\n",
        "    return str(a) + \" + \" + str(b) + \" % p = \" + str(c)\n",
        "\n",
        "def get_custom_points(indices): return [indices[-2]/2, indices[-2]/2 + p/2], [\"c/2\", \"c/2 + p/2\"]\n",
        "def get_custom_message(indices): return \"c = \" + str(indices[-2]) + \", c/2 = \" + str(indices[-2]/2) + \", c/2 + p/2 = \" + str(indices[-2]/2 + p/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c8baaf-e167-4de4-974b-18cffa414437",
      "metadata": {
        "id": "d4c8baaf-e167-4de4-974b-18cffa414437"
      },
      "source": [
        "below is a handcrafted function of length p with a strange shape with little sinusoidal content (top).\\\n",
        "the middle is a reversed version of the same function and the bottom is their sum.\\\n",
        "to seperate them on the graph, a bias has been applied afterward to the top and bottom functions.\\\n",
        "if you scroll through axis 0, the reversed version of the function is shifted prior to the summation.\\\n",
        "notice that there are actually two symmetry points. one is at shift/2 and the other at shift/2 + p/2.\\\n",
        "this is the case for all activations in the model post-attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da1d3023-7857-45ac-82af-91211b1632e1",
      "metadata": {
        "id": "da1d3023-7857-45ac-82af-91211b1632e1"
      },
      "outputs": [],
      "source": [
        "showVector(get_reversed_shifted_waves(make_custom_wave(device)), get_points=get_custom_points, get_message=get_custom_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112fe20c-cd70-4907-bea6-4be03c1ebcc6",
      "metadata": {
        "id": "112fe20c-cd70-4907-bea6-4be03c1ebcc6"
      },
      "source": [
        "below is an illustration of the structure of the activations simplified to unit magnitude cosines of the key frequencies.\\\n",
        "the shape is [frequency,c,a,b]. frequency includes a normalized sum in the last row.\\\n",
        "notice that if you scroll the c axis, a and b remain symmetrically placed around c/2.\\\n",
        "scrolling fully through the a axis, a and b wrap around, maintaining symmetry with respect to both c/2 and c/2 + p/2 in a circular fashion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e56ab30a-0c5f-46da-8b52-931c9898f151",
      "metadata": {
        "id": "e56ab30a-0c5f-46da-8b52-931c9898f151"
      },
      "outputs": [],
      "source": [
        "showVector(r_model, start_play_axis=1, get_points=get_cos_points, get_message=get_cos_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dcda7b7-6cde-4366-b143-97740c0b9265",
      "metadata": {
        "id": "7dcda7b7-6cde-4366-b143-97740c0b9265"
      },
      "source": [
        "\n",
        "\n",
        "to get this done, the model forms cosine waves over the inputs centered at c/2\\\n",
        "there are a number (~5) of key frequencies (key_freqs) for which these waves form\\\n",
        "each residual stream dimension forms it's own composite wave made up of len(key_freqs) cosines with different amplitudes,/\n",
        "but they are all symmetric about c/2.\\\n",
        "given that cosine is an even function and a and b are both equidistant from c/2, the model forms two symmetry points\\\n",
        "one is at c/2 and the other is at c/2 + p/2, allowing the modular addition to wrap around the inputs in a periodic fashion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd25f50-176d-402c-9c8b-96c51047fc57",
      "metadata": {
        "id": "2dd25f50-176d-402c-9c8b-96c51047fc57"
      },
      "source": [
        "there is more later that shows pattern weights for a & b are always equal with (1 - their sum) going to the \"=\"./\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14fb5dc4-63c1-4867-bad1-252f51f829df",
      "metadata": {
        "id": "14fb5dc4-63c1-4867-bad1-252f51f829df"
      },
      "source": [
        "## Transformations of Waves"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3090b7e8-6266-42d5-8715-a45a9f2e6231",
      "metadata": {
        "id": "3090b7e8-6266-42d5-8715-a45a9f2e6231"
      },
      "source": [
        "most of the computations done by the model are linear transformation.\\\n",
        "in this work and the previous work, we are analyzing activations of the entire dataset./\n",
        "it is important to understand how sinusoids indexed by token position are affected by linear transformations.\\\n",
        "\n",
        "a linear map, in this case, takes a number of different (but typically similar) waves and weights them.\\\n",
        "each individual weight can only do two operations. it can scale the input wave and/or negate the input wave.\\\n",
        "after the weights are applied all these scaled/negated waves are summed into a single output dimension.\\\n",
        "this process is done for every dimension of the output.\\\n",
        "\\\n",
        "this gives the model the ability to manipulate the shape of these waves and adjust the magnitude and phase of the spectra in certain ways.\\\n",
        "for instance, the model can increase a certain frequency by applying large, positive weights to the waves that include that frequency\\\n",
        "and are in-phase with eachother while negating the weights for waves that are out-of-phase.\\\n",
        "frequencies can be attenuated with this mechanism by only negating waves that are in-phase\n",
        "in this sense, a linear map can be seen as a set of multi-channel linear-phase filters, each with one coefficient that sum at their output.\\\n",
        "\\\n",
        "this is on full display in the MLP. the W_in weights with respect to each neuron effectively act as a narrowband filter.\\\n",
        "since each weight can only scale and/or negate, there is obvious learned coordination to filter these neurons by frequency.\\\n",
        "for each neuron, there is one input weight for each residual stream dimension. to filter selectively for a desired frequency and phase,\\\n",
        "the neuron assigns a weight that is a cross correlation measure between the desired wave and the content at that residual stream dimension.\\\n",
        "\n",
        "that supply each neuron with a wave of a single frequency (one of the key_freqs) and phase.\\\n",
        "this gives the model a palette of waves for each of the 5 key frequencies, with a select span of phases to draw from for the MLP output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02143d61-7d29-4735-898c-3634589d208d",
      "metadata": {
        "id": "02143d61-7d29-4735-898c-3634589d208d"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a8bacefa-8661-43ca-b7eb-a9d17c4775d0",
      "metadata": {
        "id": "a8bacefa-8661-43ca-b7eb-a9d17c4775d0"
      },
      "source": [
        "here you can view a simplified synthetic model of the residual stream. the axes are [ frequency, c, a, b ]\\\n",
        "the d_model axis is reduced and the first axis splits the key_freqs up with the last element being a normalized sum\\\n",
        "if you scroll through the c or a axes, a and b remain located at circularly equidistant positions about both c/2 and c/2 + p/2\\\n",
        "also, the cosine wave remains centered at c/2 at any indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a94e00a7-0f87-42db-8946-8d890e9fd6f3",
      "metadata": {
        "id": "a94e00a7-0f87-42db-8946-8d890e9fd6f3"
      },
      "source": [
        "this same scheme can be plotted for any activation starting with the attention output and the symmetries will remain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b1a41f-1012-4285-a256-59bf47991cef",
      "metadata": {
        "id": "84b1a41f-1012-4285-a256-59bf47991cef"
      },
      "source": [
        "the only difference between the above model and reality, is that for each \"c\", the final residual stream \\\n",
        "has a weight and bias applied to each of the cosine waves, but the symmetry and phase remain identical\\\n",
        "\\\n",
        "here is the actual final residual stream. the axes are [d_model,c,a]\\\n",
        "scrolling through the c axis, you'll see that as c increases, symmetries remain at c/2 and c/2 + p/2\\\n",
        "the x axis represents \"a\", when you scroll \"c\", element 0 is always the same, mirrored with element at index c, since 0 + c = c\\\n",
        "this means that c = b, in this case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be138baa-541b-4346-a67d-9e8acde181a9",
      "metadata": {
        "id": "be138baa-541b-4346-a67d-9e8acde181a9"
      },
      "outputs": [],
      "source": [
        "def get_top_freqs_and_indices(hook_x, dim=-1, *, sumlist=[-2], freqs_allowed=key_freqs):\n",
        "    freqs, mags = get_top_k_freqs(hook_x, 1, dim, sumlist=sumlist, freqs_allowed=freqs_allowed, squeeze=True)\n",
        "    idx_dict = {}\n",
        "    dim_range = torch.arange(freqs.numel(), device=freqs.device)\n",
        "    for freq in freqs.unique().tolist(): idx_dict[freq] = dim_range[freqs == freq]\n",
        "    freq_idx_sorted = freqs.sort()[1]\n",
        "    return freqs, mags, idx_dict, freq_idx_sorted\n",
        "\n",
        "neuron_freqs, _, neuron_freq_idx, neuron_freq_idx_sorted = get_top_freqs_and_indices(inputs_last(cache[\"pre\"])[-1])\n",
        "\n",
        "test_freq = key_freqs[0].item()\n",
        "num_neur = len(neuron_freq_idx[test_freq])\n",
        "freq_pre = inputs_last(cache[\"pre\"])[-1,neuron_freq_idx[test_freq]]\n",
        "\n",
        "neuron_phases = torch.angle(torch.fft.fft(freq_pre[...,0,:])[...,test_freq])\n",
        "_, ordered_phase_idx = torch.sort(neuron_phases)#, descending=True)\n",
        "\n",
        "# freq_pre = freq_pre[ordered_phase_idx]\n",
        "\n",
        "mock_freq_pre, mock_phases = torch.zeros([num_neur,p,p], device=device), torch.arange(num_neur, device=device) * 2 * torch.pi/num_neur - torch.pi\n",
        "wk = test_freq * 2 * torch.pi / p\n",
        "for d in range(num_neur):\n",
        "    mock_freq_pre[d] = torch.cos(neuron_phases[d] + prange[...,None] * wk) + torch.cos(neuron_phases[d] + prange[None] * wk)\n",
        "\n",
        "# mock_freq_pre = mock_freq_pre[inv_idx(ordered_phase_idx)]\n",
        "\n",
        "def running_sum_with_and_without_relu(pre, weights=cache[\"W_out\"][neuron_freq_idx[test_freq],0]):\n",
        "    non_relu_list = running_sum(pre,0, weights=weights, create_normalized_list=True)\n",
        "    post = F.relu(pre)\n",
        "    relu_list = running_sum(post,0, weights=weights, create_normalized_list=True)\n",
        "    max_mag = relu_list[0].abs().max()\n",
        "    return [relu_list[0] + max_mag, relu_list[1] + max_mag, non_relu_list[0] - max_mag, non_relu_list[1] - max_mag]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1b5dfe-bd20-4863-a15c-318e3ec5b152",
      "metadata": {
        "id": "4a1b5dfe-bd20-4863-a15c-318e3ec5b152"
      },
      "source": [
        "here we have the pre-activation for every neuron whose highest magnitude frequency is the first of the key_freqs.\\\n",
        "notice the movement of the waves as you scroll through the a axis.\\\n",
        "each neuron is approximately: cos$\\omega_{neur}$a+$\\phi_{neur}$ + cos$\\omega_{neur}$b+$\\phi_{neur}$  - shape: [neuron, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6372cffe-3a33-4b5e-9bb9-1b586e026c8a",
      "metadata": {
        "id": "6372cffe-3a33-4b5e-9bb9-1b586e026c8a"
      },
      "outputs": [],
      "source": [
        "showVector(freq_pre, start_play_axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d8f3c2-1db0-459d-813b-217b022dc63f",
      "metadata": {
        "id": "d9d8f3c2-1db0-459d-813b-217b022dc63f"
      },
      "source": [
        "here we have a synthetic tensor made to reflect the neurons above.  it is an idealized set of unit 2D waves,\\\n",
        "equally spread across all phases ordered and spaced from -$\\pi$ to $\\pi$.  - shape: [neuron, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbfec2dd-13b0-436e-95a4-910d19a51434",
      "metadata": {
        "id": "cbfec2dd-13b0-436e-95a4-910d19a51434"
      },
      "outputs": [],
      "source": [
        "showVector(mock_freq_pre)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "209f98f7-1ae3-46da-bfea-f1c339031957",
      "metadata": {
        "id": "209f98f7-1ae3-46da-bfea-f1c339031957"
      },
      "source": [
        "below we use the handcrafted model above to demonstrate the affect of the rELU on the neurons for this frequency.\\\n",
        "on the top of the graph are the rELU activated mock neurons, the bottom are the mock neurons without the rELU.\\\n",
        "the first axis scrolls through each mock neuron (orange = pre-relu, magenta = post-relu).\\\n",
        "the cyan and red are a running sum (cyan = pre-relu, red = post-relu).\\\n",
        "the last neuron element will illustrate the sum of all neurons.\\\n",
        "notice that the relu cuts off the bottom of the pre-activated mock neuron,\\\n",
        "allowing the rectified wave to travel all the way from left to right from a = 0 to a = 112.\\\n",
        "this is what gives the model the ability to dot with the unembed matrix from any a or b position.  - shape: [neuron, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd84e68-2a6b-4714-9c76-95126dbaf861",
      "metadata": {
        "id": "6fd84e68-2a6b-4714-9c76-95126dbaf861"
      },
      "outputs": [],
      "source": [
        "showVector(running_sum_with_and_without_relu(mock_freq_pre, weights=None), start_indices=[43], start_play_axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "110763c8-d63f-4e2a-9d07-27509e912305",
      "metadata": {
        "id": "110763c8-d63f-4e2a-9d07-27509e912305"
      },
      "source": [
        "below are the actual neuron activations for this frequency.  - shape: [neuron, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af08bee7-6702-449e-9341-9900dc591389",
      "metadata": {
        "id": "af08bee7-6702-449e-9341-9900dc591389"
      },
      "outputs": [],
      "source": [
        "showVector(running_sum_with_and_without_relu(freq_pre), start_indices=[43], start_play_axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60a939a5-6443-4041-bafc-4f1dcc9f35db",
      "metadata": {
        "id": "60a939a5-6443-4041-bafc-4f1dcc9f35db"
      },
      "outputs": [],
      "source": [
        "tprint(cache[\"W_out\"][neuron_freq_idx[test_freq],0].sort()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c62ea2-b31c-442a-a4e1-56d225039d70",
      "metadata": {
        "id": "f2c62ea2-b31c-442a-a4e1-56d225039d70"
      },
      "outputs": [],
      "source": [
        "# showVector(running_sum_with_and_without_relu(inputs_last(cache[\"pre\"])[-1,neuron_freq_idx[test_freq]] * cache[\"W_out\"][neuron_freq_idx[test_freq],4][...,None,None] + cache[\"b_out\"][4,None,None]), start_indices=[43], start_play_axis=1)\n",
        "showVector(einops.einsum(inputs_last(F.relu(cache[\"pre\"]))[-1,neuron_freq_idx[test_freq]],cache[\"W_out\"][neuron_freq_idx[test_freq]], \"neur posa posb, neur d_model -> d_model posa posb\") + cache[\"b_out\"][...,None,None], start_indices=[43], start_play_axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cb4b5fc-d042-4e27-9100-c9e9cfa7e493",
      "metadata": {
        "id": "2cb4b5fc-d042-4e27-9100-c9e9cfa7e493"
      },
      "source": [
        "next is the last position of the final residual stream with all frequencies zeroed except this one.  - shape: [d_model, a, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312818fa-ea4a-4da4-84d0-1711de4c34fa",
      "metadata": {
        "id": "312818fa-ea4a-4da4-84d0-1711de4c34fa"
      },
      "outputs": [],
      "source": [
        "showVector(pull_out_freqs(inputs_last(cache[\"mlp_out\"])[-1], test_freq, dims=[-2,-1]), start_indices=[1], start_play_axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268b8821-3000-43f8-958e-b2fc86051d38",
      "metadata": {
        "id": "268b8821-3000-43f8-958e-b2fc86051d38"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e37f486-9552-48c4-9512-c586efa08a19",
      "metadata": {
        "id": "2e37f486-9552-48c4-9512-c586efa08a19"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# check for magnitude of second order frequencies vs key frequencies\n",
        "def get_second_order_freq_proportion(input):\n",
        "    x = inputs_last(input)\n",
        "    XA, XB = torch.fft.fft(x), torch.fft.fft(x, x.size(-2), -2)\n",
        "    XA, XAH, XB, XBH = XA[...,key_freqs], XA[...,key_harmonics], XB[...,key_freqs,:], XB[...,key_harmonics,:]\n",
        "    mags1a, mags2a, mags1b, mags2b = XA.abs() , XAH.abs(), XB.abs() , XBH.abs()\n",
        "    other_dims_a = tuple([i for i in range(x.ndim) if i != x.ndim - 1])\n",
        "    other_dims_b = tuple([i for i in range(x.ndim) if i != x.ndim - 2])\n",
        "    proportion = (mags2a.sum(other_dims_a) / mags1a.sum(other_dims_a) + mags2b.sum(other_dims_b) / mags1b.sum(other_dims_b))/2\n",
        "    # tprint(name, other_dims, proportion)\n",
        "    return proportion\n",
        "\n",
        "def get_dims_by_second_order_freq_proportion(input, freq_idx):\n",
        "    freq, harmonic = key_freqs[freq_idx], key_harmonics[freq_idx]\n",
        "    x = inputs_last(input)\n",
        "    XA, XB = torch.fft.fft(x), torch.fft.fft(x, x.size(-2), -2)\n",
        "    XA, XAH, XB, XBH = XA[...,freq], XA[...,harmonic], XB[...,freq,:], XB[...,harmonic,:]\n",
        "    mags1a, mags2a, mags1b, mags2b = XA.abs() , XAH.abs(), XB.abs() , XBH.abs()\n",
        "    other_dims_a = tuple([i for i in range(XA.ndim) if i > 0 and i != XA.ndim - 1])\n",
        "    other_dims_b = tuple([i for i in range(x.ndim) if i > 0 and i != x.ndim - 2])\n",
        "    proportion = (mags2a.sum(-1) / mags1a.sum(-1) + mags2a.sum(-1) / mags1a.sum(-1))/2\n",
        "    tprint(\"freq\", freq, \"harmonic\", harmonic, \"proportion\", proportion.shape)\n",
        "    mags, idx = proportion.sort(0, True)\n",
        "    tprint(\"mags[:10]\", mags[:10], \"idx[:10]\", idx[:10])\n",
        "    # tprint(name, other_dims, proportion)\n",
        "    return idx\n",
        "\n",
        "tprint(\"key_freqs\", key_freqs)\n",
        "tprint(\"key_harms\", key_harmonics)\n",
        "\n",
        "# for hook_name in [\"resid_pre\", \"pattern\", \"resid_mid\", \"mlp_out\", \"resid_post\"]:\n",
        "#     tprint(hook_name, get_second_order_freq_proportion(cache[hook_name], hook_name))\n",
        "\n",
        "tprint(\"resid_pre\", get_second_order_freq_proportion(inputs_last(cache[\"resid_pre\"])[:2]))\n",
        "tprint(\"attn_out\", get_second_order_freq_proportion(inputs_last(cache[\"attn_out\"])[-1]))\n",
        "tprint(\"resid_mid\", get_second_order_freq_proportion(inputs_last(cache[\"resid_mid\"])[-1]))\n",
        "tprint(\"pre\", get_second_order_freq_proportion(inputs_last(cache[\"pre\"])[-1]))\n",
        "# tprint(\"post\", get_second_order_freq_proportion(inputs_last(cache[\"post\"])[-1]))\n",
        "tprint(\"mlp_out\", get_second_order_freq_proportion(inputs_last(cache[\"mlp_out\"])[-1]))\n",
        "tprint(\"resid_post\", get_second_order_freq_proportion(inputs_last(cache[\"resid_post\"])[-1]))\n",
        "second_idx = get_dims_by_second_order_freq_proportion(inputs_last(cache[\"post\"])[-1,neuron_freqs == 35], 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a88e9da6-3978-45ec-be78-f2c1d36101d8",
      "metadata": {
        "id": "a88e9da6-3978-45ec-be78-f2c1d36101d8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "mlp_out_freqs = torch.zeros([p*p,len(key_freqs) + 1,d_model], device=device)\n",
        "mlp_out_freqs[:,-1] += cache[\"b_out\"][None]\n",
        "# for f, freq in enumerate(key_freqs):\n",
        "#     mlp_out_freq = einops.einsum(cache[\"post\"][:,-1,neuron_freqs == freq], cache[\"W_out\"][neuron_freqs == freq], \"b m, m d -> b d\")\n",
        "#     mlp_out_freqs[:,f] = mlp_out_freq\n",
        "#     mlp_out_freqs[:,-1] += mlp_out_freq\n",
        "\n",
        "rslist = running_sum(cache[\"post\"][a_idx][:,-1,neuron_freq_idx_sorted] * cache[\"W_out\"][neuron_freq_idx_sorted,0],-1, create_normalized_list=True)\n",
        "rslistpre = running_sum(cache[\"pre\"][a_idx][:,-1,neuron_freq_idx_sorted] * cache[\"W_out\"][neuron_freq_idx_sorted,0],-1, create_normalized_list=True)\n",
        "tprint(\"neuron_freqs\", neuron_freqs[:10], \"neuron_freq_idx_sorted\", neuron_freq_idx_sorted[:10])\n",
        "rslist[0] += 5\n",
        "rslist[1] += 5\n",
        "rslistpre[0] -= 5\n",
        "rslistpre[1] -= 5\n",
        "# showVector(c_hook(\"post\"))\n",
        "# showVector(rslist + rslistpre)\n",
        "# showVector(inputs_last(cache[\"mlp_out\"])[-1])\n",
        "# showVector(inputs_last(mlp_out_freqs))\n",
        "# analyzing freq 14 neuron phases\n",
        "\n",
        "freq = 14\n",
        "hook_post = inputs_last(cache[\"post\"][c_idx])[-1,neuron_freqs == freq]\n",
        "hook_post_mags, hook_post_phases = torch.abs(torch.fft.fft(hook_post)[...,freq]), torch.angle(torch.fft.fft(hook_post)[...,freq])\n",
        "clast, cmin, cmax = torch.zeros_like(hook_post_phases[:10,0]), torch.zeros_like(hook_post_phases[:10,0]), torch.zeros_like(hook_post_phases[:10,0])\n",
        "tprint(\"clast\", clast.shape)\n",
        "def get_phase_distance(p1, p2):\n",
        "    pmin, pmax = torch.minimum(p1,p2), torch.maximum(p1,p2)\n",
        "    pdiff = pmax - pmin\n",
        "    return torch.where(pdiff > torch.pi, pmin + 2 * torch.pi - pmax, pdiff)\n",
        "\n",
        "\n",
        "clast = hook_post_phases[:,0]\n",
        "for c in range(1,100):\n",
        "    # for d in range(hook_post.size(0)):\n",
        "    W_out_polarity = cache[\"W_out\"][neuron_freqs == freq,0] > 0\n",
        "    round_phases, counts = torch.round(hook_post_phases[:,c], decimals=1).unique(return_counts=True)\n",
        "    # tprint(c, \"rounded phases\", round_phases, counts)\n",
        "    dist = get_phase_distance(clast, hook_post_phases[:,c])\n",
        "    clast = hook_post_phases[:,c]\n",
        "    # tprint(\"c\", c, \"dist\", torch.nonzero(dist > .6), \"phases\", hook_post_phases[torch.nonzero(dist > 0.6),c])\n",
        "    the_mode = torch.mode(torch.round(hook_post_phases[:,c], decimals=1))\n",
        "    # tprint(\"c\", c, \"polarity/phases\", torch.cat((W_out_polarity[:10,None],hook_post_phases[:10,c:c+1]), -1))\n",
        "\n",
        "# hook_pre = einops.einsum(cache[\"resid_mid\"], cache[\"W_in\"], \"batch pos d_model, d_model d_mlp -> batch pos d_mlp\") + cache[\"b_in\"]\n",
        "# tprint(\"pre close\", torch.allclose(hook_pre, cache[\"pre\"]))\n",
        "# hook_post = F.relu(hook_pre)\n",
        "# non_relu_mlp_out = einops.einsum(cache[\"pre\"], cache[\"W_out\"], \"batch pos d_mlp, d_mlp d_model -> batch pos d_model\") + cache[\"b_out\"]\n",
        "# non_relu_mlp_out2[non_relu_mlp_out2 > 0] = 0\n",
        "hook_post = F.relu(cache[\"pre\"].clone())\n",
        "c_post = inputs_last(hook_post)[-1]\n",
        "# for d in range((neuron_freqs == freq).size(0)):\n",
        "#     neuron_outputs = cache[\"W_out\"][neuron_freqs == freq]\n",
        "#     tprint(d, \"phase\", hook_post_phases[d][0], \"outputs\", neuron_outputs[d,0])\n",
        "# mlp_out = einops.einsum(hook_post, cache[\"W_out\"], \"batch pos d_mlp, d_mlp d_model -> batch pos d_model\") + cache[\"b_out\"]\n",
        "# non_relu_mlp_out2 = einops.einsum(cache[\"pre\"].clone(), cache[\"W_out\"], \"batch pos d_mlp, d_mlp d_model -> batch pos d_model\") + cache[\"b_out\"]\n",
        "# mlp_out = inputs_last(mlp_out[a_idx])[-1]\n",
        "# non_relu_mlp_out2 = inputs_last(non_relu_mlp_out2[a_idx])[-1]\n",
        "# # non_relu_mlp_out = pull_out_freqs(non_relu_mlp_out, key_freqs)\n",
        "# # showVector([mlp_out, non_relu_mlp_out2])\n",
        "# # showVector(inputs_last(cache[\"mlp_out\"][a_idx])[-1])\n",
        "\n",
        "# fake_mlp = torch.zeros_like(non_relu_mlp_out[0])\n",
        "# fake_mlp = F.relu(torch.cos(10 + (prange[...,None] + prange[None]) * 14 * 2 * torch.pi / p) + torch.cos(10 +(prange[...,None] - prange[None]) * 14 * 2 * torch.pi / p))\n",
        "# fake_mlp += F.relu(torch.cos(11 + (prange[...,None] + prange[None]) * 14 * 2 * torch.pi / p) + torch.cos(11 +(prange[...,None] - prange[None]) * 14 * 2 * torch.pi / p))\n",
        "# fake_mlp += F.relu(torch.cos(12 + (prange[...,None] + prange[None]) * 14 * 2 * torch.pi / p) + torch.cos(12 +(prange[...,None] - prange[None]) * 14 * 2 * torch.pi / p))\n",
        "# fake_mlp[fake_mlp < 0] = 0\n",
        "# fake_mlp = F.relu(fake_mlp)\n",
        "# showVector(fake_mlp)\n",
        "\n",
        "# hook_post_mags, hook_post_mag_idx = hook_post_mags.sort(-1, True)\n",
        "\n",
        "# tprint(\"hook 14 neuron phases\", hook_post_phases.sort()[0])\n",
        "# ccc = hook_post_phases\n",
        "# cccmags = hook_post_mags\n",
        "# showVector([ccc[hook_post_mag_idx][None], cccmags[None]/30])\n",
        "# tprint(\"central phase\", get_central_phase(hook_post[...,0,:], freq))\n",
        "# showVector(cccmags[None])\n",
        "\n",
        "# tprint(\"b_in min max\", cache[\"b_in\"].min(), cache[\"b_in\"].max())\n",
        "# tprint(\"b_out min max\", cache[\"b_out\"].min(), cache[\"b_out\"].max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd82fbc0-fc4f-4023-975c-61704ccd2ad8",
      "metadata": {
        "id": "cd82fbc0-fc4f-4023-975c-61704ccd2ad8"
      },
      "outputs": [],
      "source": [
        "showVector(inputs_last(cache[\"mlp_out\"])[-1])\n",
        "showVector(zero_out_freqs(inputs_last(cache[\"mlp_out\"])[-1], [2,3,4,5,6,7,8,9,10,11,12,13,27,28,29,30,31,32,33,34,36,37,38,39,40,43,44,45,55,56]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72927a20-2903-48f4-a47c-c6d2d4de8d69",
      "metadata": {
        "id": "72927a20-2903-48f4-a47c-c6d2d4de8d69"
      },
      "source": [
        "All linear maps can be viewed in the lens of FIR (Finite Impulse Response) filters.  Specifically, the weight at each output dimension of a linear map can be seen as a filter that takes in a multi-channel input signal and applies a seperate one-point FIR filter to each channel before summing these channels to produce its output.  Though the length of this filter is one, the width (number of channels) of the filter is the dimensionality of the vector space.\\\n",
        "\\\n",
        "Through this abstract lens, the input signals (one for each dimension) are not a function of time, but a function of the index into the input axis of the embedding matrix's vector space.  For GPT2, this is less relavent, but for this model, the input index ordering is very meaningful.\\\n",
        "\\\n",
        "For each model dimension of this model (d_model, d_mlp, d_head...) , the activations for each input represent the phase of a wave, each of length 2$\\pi.$  During the forward pass, these waves are seen in the activations and originate in the embeddings and were learned during training through back-propagation.\\\n",
        "\\\n",
        "The"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7800feb3-a1b4-4adb-845a-0a07803c9035",
      "metadata": {
        "id": "7800feb3-a1b4-4adb-845a-0a07803c9035"
      },
      "source": [
        "the rest of this analysis will focus on how the attention mechanism produces these symmetries..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7baf9b8b-6353-436c-a4f9-bfd438769c98",
      "metadata": {
        "id": "7baf9b8b-6353-436c-a4f9-bfd438769c98"
      },
      "source": [
        "to understand how the model performs modular addition, the best place to start is with the value vectors,\\\n",
        "which is the original residual stream with a \"W_V\" map applied.\\\n",
        "if you toggle the first (position) axis below and look carefully, you'll notice that positions \"a\" and \"b\" are reversed versions of eachother.\\\n",
        "if you scroll through the \"c\" axis, you'll see the same thing only circularly shifted by c\\\n",
        "shape is [pos,head,d_head,c,a]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d04e28-b9ff-490a-9119-6cb35d7091c2",
      "metadata": {
        "id": "a6d04e28-b9ff-490a-9119-6cb35d7091c2"
      },
      "outputs": [],
      "source": [
        "showVector(inputs_last(cache[\"v\"][c_idx]))\n",
        "showVector(inputs_last(cache[\"v\"][a_idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ebbf43-ee75-4c40-9285-4b10307fc39a",
      "metadata": {
        "id": "36ebbf43-ee75-4c40-9285-4b10307fc39a"
      },
      "outputs": [],
      "source": [
        "for name in cache: tprint(name, cache[name].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf054dd5-dc37-4731-8290-427b460d7fff",
      "metadata": {
        "id": "bf054dd5-dc37-4731-8290-427b460d7fff"
      },
      "outputs": [],
      "source": [
        "# showVector(cache[\"unembed.W_U\"])\n",
        "wu = cache[\"unembed.W_U\"][...,None].expand(d_model,p,p)\n",
        "wu = wu / wu.var().sqrt()\n",
        "tprint(\"wu\", wu.shape, \"var\", wu.var())\n",
        "\n",
        "def show_wu_act(name):\n",
        "    resid = inputs_last(cache[name][a_idx])\n",
        "    # resid_mid = c_hook(\"resid_mid\")\n",
        "    resid = resid / resid.var().sqrt()\n",
        "    tprint(name, resid.shape, \"var\", resid.var())\n",
        "    showVector([resid[-1],wu[None].expand_as(resid)[-1]])\n",
        "    # showVector(resid[-1])\n",
        "\n",
        "def show_attn_act(name):\n",
        "    resid = inputs_last(cache[name][a_idx])\n",
        "    # resid_mid = c_hook(\"resid_mid\")\n",
        "    resid = resid / resid.var().sqrt()\n",
        "    tprint(name, resid.shape, \"var\", resid.var())\n",
        "    showVector(resid)\n",
        "\n",
        "\n",
        "show_wu_act(\"resid_mid\")\n",
        "show_wu_act(\"attn_out\")\n",
        "show_attn_act(\"z\")\n",
        "show_wu_act(\"resid_post\")\n",
        "\n",
        "# resid_mid = inputs_last(cache[\"resid_mid\"])\n",
        "# # resid_mid = c_hook(\"resid_mid\")\n",
        "# resid_mid = resid_mid / resid_mid.var().sqrt()\n",
        "# tprint(\"resid_mid\", resid_mid.shape, \"var\", resid_mid.var())\n",
        "# showVector([wu[None].expand_as(resid_mid),resid_mid])\n",
        "\n",
        "# resid_post = inputs_last(cache[\"resid_post\"])\n",
        "# # resid_post = c_hook(\"resid_post\")\n",
        "# resid_post = resid_post / resid_post.var().sqrt()\n",
        "# tprint(\"resid_post\", resid_post.shape, \"var\", resid_post.var())\n",
        "# showVector([wu[None].expand_as(resid_mid),resid_post])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19659ae8-2c3d-41b6-826e-1fa174d1d9d6",
      "metadata": {
        "id": "19659ae8-2c3d-41b6-826e-1fa174d1d9d6"
      },
      "outputs": [],
      "source": [
        "x = cache[\"resid_pre\"]\n",
        "W_Q, W_K, W_V = cache[\"W_Q\"], cache[\"W_K\"], cache[\"W_V\"]\n",
        "q = einops.einsum(W_Q, x, \"i d h, b p d -> b p i h\")# [nh C hs] @ [B T C] = [B T nh hs]\n",
        "k = einops.einsum(W_K, x, \"i d h, b p d -> b p i h\")# [nh C hs] @ [B T C] = [B T nh hs]\n",
        "v = einops.einsum(W_V, x, \"i d h, b p d -> b p i h\")# [nh C hs] @ [B T C] = [B T nh hs]\n",
        "q,k,v = cache[\"q\"], cache[\"k\"], cache[\"v\"]\n",
        "# showVector(torch.stack((c_hook(v), c_hook(cache[\"v\"])/30.0)))\n",
        "q = einops.rearrange(q, \"b p i h -> b p i h\")\n",
        "k = einops.rearrange(k, \"b p i h -> b p i h\")\n",
        "v = einops.rearrange(v, \"b p i h -> b p i h\")\n",
        "q = q.transpose(1,2)# [B T nh hs] -> [B nh T hs]\n",
        "k = k.transpose(1,2).transpose(-2, -1)# [B T nh hs] -> [B nh T hs] -> [B nh hs T]\n",
        "v = v.transpose(1,2)\n",
        "\n",
        "attn_scores = torch.matmul(q, k)/np.sqrt(d_head)# [B nh T hs] @ [B nh hs T] = [B nh T T]\n",
        "attn_scores_masked = attn_scores + torch.full([1, 3, 3], -torch.inf).triu(1).to(k.device)# [B nh T T]\n",
        "pattern = F.softmax(attn_scores_masked, -1)# [B nh T T]\n",
        "z = einops.einsum(v, pattern, \"batch mhead k_pos d_head, batch mhead q_pos k_pos -> batch mhead q_pos d_head\")\n",
        "hook_z = einops.rearrange(z, \"batch head q_pos d_head -> batch q_pos head d_head\", head=n_heads)\n",
        "out = einops.einsum(hook_z, cache[\"W_O\"], \"batch pos head d_head, head d_head d_model -> batch pos d_model\")\n",
        "\n",
        "resid_mid = out + cache[\"resid_pre\"]\n",
        "W_in, W_out, b_in, b_out = cache[\"W_in\"], cache[\"W_out\"], cache[\"b_in\"], cache[\"b_out\"]\n",
        "hook_pre = einops.einsum(resid_mid, W_in, \"batch pos d_model, d_model d_mlp -> batch pos d_mlp\") + b_in\n",
        "hook_post = F.relu(hook_pre)\n",
        "mlp_out = einops.einsum(hook_post, W_out, \"batch pos d_mlp, d_mlp d_model -> batch pos d_model\") + b_out\n",
        "resid_post = resid_mid + mlp_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "248bfa0b-6983-4517-9f92-9d6f5282509a",
      "metadata": {
        "id": "248bfa0b-6983-4517-9f92-9d6f5282509a"
      },
      "outputs": [],
      "source": [
        "c_post = c_hook(\"post\")[-1,:,0]\n",
        "C_POST = torch.fft.fft(c_post)/p\n",
        "C_MAGS = C_POST[...,:p//2+1].abs()\n",
        "C_MAG_SUM = C_MAGS.sum(0)\n",
        "tprint(\"C_MAG_SUM top 20\", torch.cat((C_MAG_SUM.topk(20)[1][...,None], C_MAG_SUM.topk(20)[0][...,None]), -1))\n",
        "post_dc = (2 * C_MAGS[:,:1]).clone()\n",
        "C_MAGS[...,0] = 0\n",
        "post_mags, post_freqs = C_MAGS.topk(2)\n",
        "tprint(\"post_freqs shape\", post_freqs.shape)\n",
        "row_indices = torch.arange(d_mlp, device=device).unsqueeze(1).expand(-1, 2)\n",
        "post_phases = torch.angle(C_POST[row_indices, post_freqs])\n",
        "post_amps = C_POST[row_indices, post_freqs].real\n",
        "\n",
        "freq_dict, best_freq_dict, mag_dict, best_mag_dict = {}, {}, {}, {}\n",
        "best_mags, all_mags = torch.zeros([p], device=device), torch.zeros([p], device=device)\n",
        "for d in range(d_mlp):\n",
        "    if post_freqs[d,0].item() not in best_freq_dict: freq_dict[post_freqs[d,0].item()] = best_freq_dict[post_freqs[d,0].item()] = 0\n",
        "    if post_freqs[d,1].item() not in freq_dict: freq_dict[post_freqs[d,1].item()] = 0\n",
        "    best_freq_dict[post_freqs[d,0].item()] = best_freq_dict[post_freqs[d,0].item()] + 1\n",
        "    best_mags[post_freqs[d,0].item()] += post_mags[d,0].item()\n",
        "    all_mags[post_freqs[d,0].item()] += post_mags[d,0].item()\n",
        "    all_mags[post_freqs[d,1].item()] += post_mags[d,1].item()\n",
        "    freq_dict[post_freqs[d,0].item()] = freq_dict[post_freqs[d,0].item()] + 1\n",
        "    freq_dict[post_freqs[d,1].item()] = freq_dict[post_freqs[d,1].item()] + 1\n",
        "\n",
        "# for f in freq_dict: tprint(\"freq_dict freq\", f, \"total\", freq_dict[f])\n",
        "# for f in best_freq_dict: tprint(\"best_freq_dict freq\", f, \"total\", best_freq_dict[f], \"best mag\", best_mags[f], \"total mag\", all_mags[f], \"mag accum sum\", C_MAG_SUM[f])\n",
        "\n",
        "cos_weights = torch.zeros([d_model,p//2+1], device=device)\n",
        "row_idx = torch.arange(d_model, device=device).unsqueeze(1).expand(d_model, 2)\n",
        "\n",
        "# for d in range(d_mlp):\n",
        "#     if d < 5:\n",
        "#         tprint(\"cos_weights\", cos_weights.shape, \"post_amps\", post_amps.shape, \"W_out\", W_out.shape)\n",
        "#         tprint(\"cos_weights indexed shape\", cos_weights[row_idx, post_freqs[d]].shape)\n",
        "#         tprint(\"post_freqs\", post_freqs[d], \"post_amps\", post_amps[d], \"first 5 dims\", W_out[d,:5])\n",
        "#         tprint(\"dim\", d, \"adding this to top 2 freqs\", einops.einsum(post_amps[d], W_out[d], \"n, d_model -> d_model n\")[:5])\n",
        "#     cos_weights[row_idx, post_freqs[d]] += einops.einsum(post_amps[d], W_out[d], \"n, d_model -> d_model n\")\n",
        "\n",
        "# for d in range(5):\n",
        "#     tprint(\"d\", d, cos_weights[d,key_freqs])\n",
        "#     tprint(\"d\", d, torch.fft.fft(c_hook(\"mlp_out\")[-1,:,0]).real[d,key_freqs])\n",
        "\n",
        "# for d in range(20):\n",
        "#     tprint(d, \"freq\", post_freqs[d,0], \"mag\", post_mags[d,0], \"phase\", post_phases[d,0], \"amp\", post_amps[d,0], \"cos\", post_cos[d,0], post_amps[d,0] == post_cos[d,0])\n",
        "#     tprint(d, \"freq\", post_freqs[d,1], \"mag\", post_mags[d,1], \"phase\", post_phases[d,1], \"amp\", post_amps[d,1], \"cos\", post_cos[d,1], post_amps[d,1] == post_cos[d,1])\n",
        "\n",
        "# tprint(\"mags\", post_mags[:5])\n",
        "# tprint(\"freqs\", post_freqs[:5])\n",
        "\n",
        "resid_mid = out + cache[\"resid_pre\"]\n",
        "W_in, W_out, b_in, b_out = cache[\"W_in\"], cache[\"W_out\"], cache[\"b_in\"], cache[\"b_out\"]\n",
        "hook_pre = einops.einsum(resid_mid, W_in, \"batch pos d_model, d_model d_mlp -> batch pos d_mlp\") + b_in\n",
        "hook_post = F.relu(hook_pre)\n",
        "mlp_out = einops.einsum(hook_post, W_out, \"batch pos d_mlp, d_mlp d_model -> batch pos d_model\") + b_out\n",
        "\n",
        "def do_spectral_matmul(x, W, freqs, dim):\n",
        "    # example: x = hook_post [12769, 3, 512], W = W_out [512, 128]\n",
        "    x = torch.moveaxis(c_hook(x)[-1], 0, -1)\n",
        "    tprint(\"x\", x.shape)\n",
        "    M, N = x.size(-2), W.size(-1)\n",
        "    if x.size(-1) != W.size(-2):\n",
        "        tprint(\"you can't matmul these matrices\", \"x =\", x.shape, \"W =\", W.shape, \"...\", x.size(-1), \"!=\", W.size(-2))\n",
        "\n",
        "    # branch 1 = regular matmul\n",
        "    ret1 = x @ W\n",
        "\n",
        "    # branch 2 - spectral way\n",
        "    X = torch.fft.fft(x)[freqs]\n",
        "    ret2 = torch.fft.ifft(X @ torch.complex(W, torch.zeros_like(W))).real\n",
        "    showVector(ret1)\n",
        "    showVector(ret2)\n",
        "    return ret1\n",
        "    # cos_weights = torch.zeros([W.size(-1),len(freqs)], device=device)\n",
        "    # row_idx = torch.arange(W.size(-1), device=device).unsqueeze(1).expand(W.size(-1), len(freqs))\n",
        "    # cos_weights[row_idx, freqs.expand_as(cos_weights)] += einops.einsum(X, W, \"n, d_model -> d_model n\")\n",
        "    # c_post = c_hook(\"post\")[-1,:,0]\n",
        "    # C_POST = torch.fft.fft(c_post)/p\n",
        "    # C_MAGS = C_POST[...,:p//2+1].abs()\n",
        "    # C_MAG_SUM = C_MAGS.sum(0)\n",
        "    # tprint(\"C_MAG_SUM top 20\", torch.cat((C_MAG_SUM.topk(20)[1][...,None], C_MAG_SUM.topk(20)[0][...,None]), -1))\n",
        "    # post_dc = (2 * C_MAGS[:,:1]).clone()\n",
        "    # C_MAGS[...,0] = 0\n",
        "    # post_mags, post_freqs = C_MAGS.topk(2)\n",
        "    # tprint(\"post_freqs shape\", post_freqs.shape)\n",
        "    # row_indices = torch.arange(d_mlp, device=device).unsqueeze(1).expand(-1, 2)\n",
        "    # post_phases = torch.angle(C_POST[row_indices, post_freqs])\n",
        "\n",
        "# do_spectral_matmul(hook_post, W_out, key_freqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e24bfb-2888-4c60-802f-5241feccf71a",
      "metadata": {
        "id": "83e24bfb-2888-4c60-802f-5241feccf71a"
      },
      "outputs": [],
      "source": [
        "showVector(c_hook(\"mlp_out\")[-1])\n",
        "printCudaMemUsage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8a3022-e3ed-4609-b6da-806e5cb4b902",
      "metadata": {
        "id": "ac8a3022-e3ed-4609-b6da-806e5cb4b902"
      },
      "outputs": [],
      "source": [
        "# hooks_post = metrix.get_activations_slice(dataset, [\"blocks.0.mlp.hook_post\"], [-1,0])\n",
        "tprint('hooks_post[\"blocks.0.mlp.hook_post\"].shape', hooks_post[\"blocks.0.mlp.hook_post\"].shape)\n",
        "showVector(inputs_last(hooks_post[\"blocks.0.mlp.hook_post\"].transpose(0,1)[c_idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19fc7d45-c94b-4a81-8d5c-09c4259bc468",
      "metadata": {
        "id": "19fc7d45-c94b-4a81-8d5c-09c4259bc468"
      },
      "outputs": [],
      "source": [
        "# c_post = zero_out_freqs(c_post, torch.arange(1, p//2 + 1).to(device), -1)\n",
        "tprint(\"hook_pre\", hook_pre.shape)\n",
        "showVector(a_hook(hook_pre)[-1])\n",
        "tprint(\"hook_post\", hook_post.shape)\n",
        "showVector(a_hook(hook_post)[-1])\n",
        "# showVector(c_post)\n",
        "# showVector(c_hook(\"resid_post\")[-1])\n",
        "# showVector(cache[\"W_U\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27086a91-8b64-44fc-86a8-32bb92796cd4",
      "metadata": {
        "id": "27086a91-8b64-44fc-86a8-32bb92796cd4"
      },
      "outputs": [],
      "source": [
        "showVector(inputs_last(cache[\"resid_post\"][c_idx])[-1])\n",
        "printCudaMemUsage()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35e5968-1ba3-488c-8e18-a2bb9272ad3a",
      "metadata": {
        "id": "b35e5968-1ba3-488c-8e18-a2bb9272ad3a"
      },
      "source": [
        "as a increases (i.e. 0, 1, 2, 3...), to calculate a + b % p, b must decrement circularly (i.e. 0, 112, 111, 110...)\\\n",
        "this holds for all examples where c = 0... as you increase c, b's start index must increase as well.\\\n",
        "and since a and b are just indexing the same wave, the b wave becomes a reversed, shifted version of the a wave.\\\n",
        "an interesting thing happens when you add two reversed, shifted functions together.\\\n",
        "the resulting function is doubly symmetric with respect to two lines, one at shift/2 and one at shift/2 + length/2\\\n",
        "and if the function is sinusoidal, this leaves strictly cosine waves of double amplitude, centered at shift/2.\\\n",
        "sine, being an odd function, cancels itself out when a reversed copy is added.\\\n",
        "\\\n",
        "hook_z\\\n",
        "\\\n",
        "everything in the attention layer prior to hook_z, shares this reversal/shift scheme in how inputs index sinusoidal waves in the activations\\\n",
        "hook_z is where we first see the result of the interaction between the a and b inputs.\\\n",
        "hook_z is where we first see the symmetries that remain throughout all later activations\\\n",
        "this interaction and symmetry allows the model to generalize rather than memorize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ef1c0ec-b1e8-4883-8668-c46b91ad3dc7",
      "metadata": {
        "id": "3ef1c0ec-b1e8-4883-8668-c46b91ad3dc7"
      },
      "outputs": [],
      "source": [
        "nuscores = inputs_last(cache[\"scores\"][c_idx])\n",
        "nsa,nsb = nuscores[0,-1,0,20,13], nuscores[0,-1,1,20,7]\n",
        "tprint(\"nsa,nsb\", nsa, nsb)\n",
        "showVector(nuscores)\n",
        "# tprint(nuscores)\n",
        "w5 = torch.cos(torch.arange(p) * 5 * 2 * torch.pi / p)\n",
        "w7 = torch.cos(torch.arange(p) * 7 * 2 * torch.pi / p)\n",
        "wp = w5 * w7\n",
        "# showVector(torch.stack((w5,w7,wp)))\n",
        "# showVector(c_hook(\"z\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e33c77c-04e3-44c6-a615-d3e48d4c0c92",
      "metadata": {
        "id": "7e33c77c-04e3-44c6-a615-d3e48d4c0c92"
      },
      "source": [
        "glossary:\\\n",
        "p: 113 (modulus)\\\n",
        "k: key frequency index\\\n",
        "n_heads: number of heads(4)\\\n",
        "d_model: model dimensionality(128)\\\n",
        "d_head: head dimensionality(32)\\\n",
        "head: index for head\\\n",
        "d: index for model dimension\\\n",
        "dh: index for head dimension\\\n",
        "$freq_k$: key frequency\\\n",
        "$\\omega_k$: angular frequency ($freq_k$ * 2$\\pi$ / p)\\\n",
        "$\\phi$: phase\\\n",
        "$\\alpha$,$\\beta$: amplitude\\\n",
        "|$\\alpha$|: magnitude"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29bab2ba-36f8-449d-88aa-1d86d7fbef66",
      "metadata": {
        "id": "29bab2ba-36f8-449d-88aa-1d86d7fbef66"
      },
      "source": [
        "hook_q: waves over inputs are all the same wave with phase ($\\phi_k$) with head & d_head axes weighted by W_Q:\\\n",
        "here are positions (pos) a and b.  the axes are [pos,head,d_head,c,a]\\\n",
        "all waves in position b are reversed and shifted versions of waves in position a, shifted by -c\\\n",
        "q, k, and v activations all share this reversal/shift scheme between positions a and b.\\\n",
        "this will be key to how the attention mechanism inserts symmetries into the activations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1303fa-9718-4760-8644-96f825be8808",
      "metadata": {
        "id": "6e1303fa-9718-4760-8644-96f825be8808"
      },
      "source": [
        "here is a demonstration of how the attention scores are calculated\\\n",
        "one element of the d_head space will be selected from just one head to analyze\\\n",
        "the attention scores are calculated by q @ k.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fefad71e-00e6-4179-8c9e-5caa6966c0a4",
      "metadata": {
        "id": "fefad71e-00e6-4179-8c9e-5caa6966c0a4"
      },
      "outputs": [],
      "source": [
        "showVector([qk[0], qk[1]])\n",
        "q_a,q_b,k_a,k_b = c_hook(\"q\")[0,0,0], c_hook(\"q\")[1,0,0], c_hook(\"k\")[0,0,0], c_hook(\"k\")[1,0,0]\n",
        "score = q_a\n",
        "tprint(\"q_a\", q_a.shape, \"q_b\", q_b.shape, \"k_a\", k_a.shape, \"k_b\", k_b.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ed2da3-875c-4c9c-ad75-1b60541a2e17",
      "metadata": {
        "id": "97ed2da3-875c-4c9c-ad75-1b60541a2e17"
      },
      "source": [
        "here is a demonstration of how the attention mechanism produces the required symmetries needed to predict \"c\"\\\n",
        "the same element of the d_head space will be used for analysis\\\n",
        "since for every c, the attention scores are equal, hook_v a and b are summed together\\\n",
        "the \"=\" position is also added but it is a constant and doesn't effect the frequency content of the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed39a4be-b996-4432-b131-70670de86aa9",
      "metadata": {
        "id": "ed39a4be-b996-4432-b131-70670de86aa9"
      },
      "outputs": [],
      "source": [
        "printCudaMemUsage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1728b8-ca34-4b4a-b431-544f5cf35dfc",
      "metadata": {
        "id": "8e1728b8-ca34-4b4a-b431-544f5cf35dfc"
      },
      "outputs": [],
      "source": [
        "def get_hook_stats(name, act, freqs=key_freqs):\n",
        "    if not isinstance(freqs, torch.Tensor): freqs = torch.tensor(freqs, dtype=torch.long).to(act.device)\n",
        "    x = inputs_last(act[c_idx])\n",
        "    X = torch.fft.fft(x)\n",
        "    dc = X[...,0:1].real.repeat(1,1,1,1,len(freqs)).squeeze(0)/p\n",
        "    X = X[..., freqs]\n",
        "    mag, phase = torch.abs(X)/(p/2), torch.angle(X)\n",
        "    return torch.stack((mag, phase, dc, dc/mag), -1)\n",
        "\n",
        "def get_frequency_stats(freqs=key_freqs):\n",
        "    stats = CacheDict()\n",
        "    for name in cache.keys():\n",
        "        if \"hook\" in name and (d_model in list(cache[name].shape) or d_head in list(cache[name].shape)):\n",
        "            if name not in stats.keys():\n",
        "                stats[name] = get_hook_stats(name, cache[name], freqs)\n",
        "    return stats\n",
        "\n",
        "stats, second_order_stats = get_frequency_stats(key_freqs), get_frequency_stats(key_harmonics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c66c7b-33c8-460d-83b5-04c52b02cd38",
      "metadata": {
        "id": "f2c66c7b-33c8-460d-83b5-04c52b02cd38"
      },
      "outputs": [],
      "source": [
        "tprint(\"hook_q base phases:\", stats[\"q\"][0,0,0,0,:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8db02439-3a2d-4a71-8b67-1e5d9be7304d",
      "metadata": {
        "id": "8db02439-3a2d-4a71-8b67-1e5d9be7304d"
      },
      "source": [
        "the waves for the b position are the same but with phase negated (equivalent to either reversing the wave or negating the sine component)\\\n",
        "the waves can be calculated like this:\\\n",
        "a: $\\alpha_{head,dh}$ * cos(a$\\omega_k$) + $\\beta_{head,dh}$ * sin(a$\\omega_k$)\\\n",
        "b: $\\alpha_{head,dh}$ * cos(b$\\omega_k$) - $\\beta_{head,dh}$ * sin(b$\\omega_k$)\\\n",
        "$\\alpha_{head,dh}$ and $\\beta_{head,dh}$ are amplitudes (weights) applied to each head and head dimension by W_Q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "292f4608-862f-445d-a3bb-c26a3d686464",
      "metadata": {
        "id": "292f4608-862f-445d-a3bb-c26a3d686464"
      },
      "source": [
        "when"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074103d4-8d20-4912-bb30-919f0cd56092",
      "metadata": {
        "id": "074103d4-8d20-4912-bb30-919f0cd56092"
      },
      "outputs": [],
      "source": [
        "for name in cache: tprint(name, cache[name].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42726510-68f3-4078-9489-3634e74ecef3",
      "metadata": {
        "id": "42726510-68f3-4078-9489-3634e74ecef3"
      },
      "outputs": [],
      "source": [
        "unembed, resid = cache[\"unembed.W_U\"], inputs_last(cache[\"resid_post\"])[-1,:,0]\n",
        "showVector([unembed/unembed.norm(), resid/resid.norm()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1773b16-8054-4cbb-b45d-f8e518c80454",
      "metadata": {
        "id": "b1773b16-8054-4cbb-b45d-f8e518c80454"
      },
      "outputs": [],
      "source": [
        "showVector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c72bb9-db82-40e3-9182-bac021588665",
      "metadata": {
        "id": "96c72bb9-db82-40e3-9182-bac021588665"
      },
      "outputs": [],
      "source": [
        "wk = key_harmonics * 2 * torch.pi / p\n",
        "magnitude, phase, dc = second_order_stats[\"q\"][...,0], second_order_stats[\"q\"][...,1], second_order_stats[\"q\"][...,2]\n",
        "wk = wk.expand_as(phase)\n",
        "polarity = torch.where(torch.sign(second_order_stats[\"q\"][:,...,0:1,0:1,1]) == torch.sign(second_order_stats[\"q\"][:,0:1,0:1,0:1,0:1,1]), 1.0, -1.0)\n",
        "tprint(\"polarity\", polarity.shape, \"magnitude\", magnitude.shape)\n",
        "polarity = polarity.expand_as(magnitude)\n",
        "amplitude = polarity * magnitude\n",
        "p_indices = torch.arange(p).to(device)[None,None,None,...,None]\n",
        "p_indices = p_indices.expand_as(magnitude)\n",
        "# printvars(dc, magnitude, phase, amplitude, polarity, p_indices, wk)\n",
        "\n",
        "base_phase = phase[:,0:1,0:1,0:1,:].expand_as(phase)\n",
        "r_wave = (amplitude * torch.cos(p_indices * wk + base_phase)).sum(-1)\n",
        "r_wave = dc[...,0] + r_wave\n",
        "tprint(\"key_harmonics\", key_harmonics)\n",
        "q_corrs = get_hook_correlations(c_hook(\"q\")[...,0,:], key_harmonics, name=\"hook q\")\n",
        "print_correlation_summary(q_corrs)\n",
        "# tprint(\"q_corr keys\", q_corrs.keys())\n",
        "# tprint(\"q_corr\", q_corrs)\n",
        "# print_object_info(q_corrs)\n",
        "# print(\"end of q_corr info, individual dict items below\")\n",
        "# for q_key in q_corrs.keys():\n",
        "#     print(\"info for\", q_key)\n",
        "#     print_object_info(q_corrs[q_key])\n",
        "# showVector(c_hook(\"q\"))# - c_hook(\"q\").mean(-1, True))\n",
        "# showVector([c_hook(\"q\")[:-1,...,0,:], r_wave[:-1]])\n",
        "# showVector(r_wave)\n",
        "tprint(\"r_wave diff\", get_mse_and_worst_vector(r_wave, c_hook(\"q\")[...,0,:]))\n",
        "# showVector(pull_out_freqs(c_hook(\"pattern\"), key_harmonics))\n",
        "showVector(inputs_last(cache[\"q\"][c_idx])[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715c00c5-a783-4b67-891b-75eea4479082",
      "metadata": {
        "id": "715c00c5-a783-4b67-891b-75eea4479082"
      },
      "source": [
        "W_Q takes the residual stream embeddings and applies a weight and a bias (they are either both positive or both negative)\n",
        "when they are negative, this is equivalent to adding pi to the phase.\n",
        "the base phases are the result of each heads weighting of the waves in the embeddings during it's linear transformation.\n",
        "at c == 0, the phases at position a and b are always negated versions of eachother\n",
        "when the answer, c, increases from 0 to p - 1, the wave at the a position remains the same but the wave at the b position circularly shifts to the right one point at a time but otherwise remains identical, which is equivalent to subtracting wk(freq * 2 pi / p) from every frequency\n",
        "\n",
        "a similar story goes for the k vectors, but each head focuses on one key_freq and has different starting phases (the relationship between q/k phases is random?)\n",
        "\n",
        "the W_V operation seems entirely stochastic (you can see how random it seems by scrolling through the fourier transformed W_V), which is unintuitive.  given the theory that the attn output is just a linear combination of the value vectors weighted by the attn scores, stochasticity here seems pretty crazy, and thus the trigonometric information flow (each wave) is carried through the keys and queries (i am not at all sure of this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "789a6a20-5f4c-4476-bc6d-e9f0aa01208b",
      "metadata": {
        "id": "789a6a20-5f4c-4476-bc6d-e9f0aa01208b"
      },
      "outputs": [],
      "source": [
        "a_a, a_b = prange[...,None].expand(p,p), prange[None].expand(p,p)\n",
        "a_c, b_a, b_b, b_c, c_a, c_b, c_c = (a_a + a_b) % p, a_b, a_a, (a_a + a_b) % p, (a_b + p) % p, (prange[...,None] - a_b + p) % p, a_a\n",
        "tokens = { \"a\": { \"a\":a_a, \"b\":a_b, \"c\":a_c }, \"b\": { \"a\":b_a, \"b\":b_b, \"c\":b_c }, \"c\": { \"a\":c_a, \"b\":c_b, \"c\":c_c } }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b199a2a8-fc31-4659-bd43-530fd7e55598",
      "metadata": {
        "id": "b199a2a8-fc31-4659-bd43-530fd7e55598"
      },
      "outputs": [],
      "source": [
        "showVector(c_hook(\"resid_post\")[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6565bcd2-17bc-40bf-9097-d5305a7a0bb3",
      "metadata": {
        "id": "6565bcd2-17bc-40bf-9097-d5305a7a0bb3"
      },
      "outputs": [],
      "source": [
        "### ANALYTICALLY RECONSTRUCT RESIDUAL STREAM ###\n",
        "\n",
        "def synthesize_symmetric_hook(act, freqs, *, alpha=\"c\", seperate_freqs=False):\n",
        "    freqs = freqs.to(act.device) if isinstance(freqs, torch.Tensor) else torch.tensor(freqs, dtype=torch.long, device=act.device)\n",
        "    mean, phasors = get_phasors(act, freqs)\n",
        "    mags, phases = torch.abs(phasors), torch.angle(phasors)\n",
        "    output = (torch.zeros_like(act) + mean).unsqueeze(0)\n",
        "\n",
        "    a, b, c = tokens[alpha][\"a\"], tokens[alpha][\"b\"], tokens[alpha][\"c\"]\n",
        "\n",
        "    for f in range(freqs.shape[-1]):\n",
        "\n",
        "        freq = freqs[...,f:f+1]\n",
        "        wk = freq * 2 * torch.pi / p\n",
        "        mag, phase = mags[...,f:f+1], phases[...,f:f+1]\n",
        "\n",
        "        cos_waves = torch.cos((a - c/2) * wk)\n",
        "        phasor = mag * torch.cos(phase + (c/2) * wk)\n",
        "\n",
        "        if seperate_freqs:\n",
        "            output = torch.cat((output, torch.zeros_like(output[0:1])), 0)\n",
        "            output[-1] = cos_waves * phasor + mean\n",
        "        output[0] += cos_waves * phasor\n",
        "        # output[0] += mag * 10 * cos_waves\n",
        "\n",
        "    return output if seperate_freqs else output.squeeze(0)\n",
        "\n",
        "# showVector(cache[\"unembed.W_U\"] * 5)\n",
        "\n",
        "# post_a = inputs_last(cache[\"resid_post\"][a_idx])[2]\n",
        "# new_post_a = torch.zeros_like(post_a[...,0,:])\n",
        "# for i in range(p):\n",
        "#     new_post_a += torch.roll(post_a[...,i,:], shifts=i, dims=-1)\n",
        "# new_post_a /= p\n",
        "# showVector(new_post_a / 5)\n",
        "\n",
        "def split_dim(hook_x, freqs):\n",
        "    hook_x = inputs_last(hook_x)\n",
        "    # while hook_x.ndim > 2: hook_x = hook_x[0]\n",
        "    hook_x = hook_x[:50]\n",
        "    tprint(\"hook_x\", hook_x.shape)\n",
        "    X = torch.fft.fft(hook_x)/p\n",
        "    # tprint(\"X[:5,35]    \", X[:5,35])\n",
        "    # tprint(\"X[:5,p - 35]\", X[:5,p - 35])\n",
        "    X = X[...,freqs]\n",
        "    tprint(\"X\", X.shape)\n",
        "    X = torch.view_as_real(X)\n",
        "    tprint(\"X\", X.shape)\n",
        "    X = torch.permute(X, (2,3,0,1))\n",
        "    tprint(\"X\", X.shape)\n",
        "\n",
        "    return X\n",
        "\n",
        "def shift_exponential():\n",
        "    X = torch.zeros([2,p,p], device=device, dtype=torch.float)\n",
        "    for j in range(p):\n",
        "        X[0][j] = torch.cos(prange * 14 * 2 * torch.pi / p)\n",
        "        X[1][j] = torch.sin(prange * 14 * 2 * torch.pi / p) - 1 + 2 * j / p\n",
        "    tprint(\"X just cos and sine waves\", X.shape)\n",
        "    showVector(X)\n",
        "    X = X.transpose(0,-1).contiguous()\n",
        "    tprint(\"X before ifft\", X.shape)\n",
        "    x = torch.fft.ifft(torch.view_as_complex(X))\n",
        "    tprint(\"x after ifft\", x.shape)\n",
        "    showVector(x.real)\n",
        "\n",
        "# shift_exponential()\n",
        "\n",
        "# showVector(inputs_last(cache[\"resid_post\"][a_idx])[2])\n",
        "# split = split_dim(inputs_last(cache[\"resid_post\"][a_idx])[2], key_freqs)\n",
        "# showVector([split[:,0,:], split[:,1,:]])\n",
        "\n",
        "showVector(inputs_last(cache[\"resid_post\"][a_idx])[2])\n",
        "# showVector(split_dim(inputs_last(cache[\"resid_post\"][c_idx])[2]))\n",
        "s_hook_x = synthesize_symmetric_hook(inputs_last(cache[\"resid_post\"][a_idx]), key_harmonics, alpha=\"a\")\n",
        "showVector(s_hook_x[2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a249c6e8-3444-45fc-b2cb-0bceb23a8a82",
      "metadata": {
        "id": "a249c6e8-3444-45fc-b2cb-0bceb23a8a82"
      },
      "outputs": [],
      "source": [
        "def get_fourier_coefficient_for_specific_frequency(x, frequency, dim=-1, theSampleRate=p):\n",
        "    if dim < 0: dim = x.ndim + dim\n",
        "    length = x.size(dim)\n",
        "    jw = torch.exp(-2j * torch.arange(length, device=x.device) * torch.pi * frequency / theSampleRate)\n",
        "    while jw.ndim <= dim: jw = jw[None]\n",
        "    while jw.ndim < x.ndim: jw = jw[...,None]\n",
        "    coeff = 2 * (jw * x).sum(dim) / length\n",
        "\n",
        "    # tprint(\"mag\", torch.abs(coeff), \"phase\", torch.angle(coeff))\n",
        "    return coeff\n",
        "\n",
        "two_freqs = torch.cos(torch.arange(p) * 14.5 * 2 * torch.pi / p)[None,...,None].expand(2,p,3)# + torch.cos(torch.arange(p) * 16 * 2 * torch.pi / p)\n",
        "tprint(\"two_freqs\", two_freqs.shape)\n",
        "tprint(\"15.5 Hz\", get_fourier_coefficient_for_specific_frequency(two_freqs, 14.5, 1))\n",
        "tprint(\"16 Hz\", get_fourier_coefficient_for_specific_frequency(two_freqs, 16, 1))\n",
        "tprint(\"16.5 Hz\", get_fourier_coefficient_for_specific_frequency(two_freqs, 16.5, 1))\n",
        "tprint(\"7 Hz\", get_fourier_coefficient_for_specific_frequency(two_freqs, 7, 1))\n",
        "\n",
        "vecplen = torch.cos(1.0 + torch.arange(p) * 17.5 * 2 * torch.pi / p)\n",
        "vecp2len = torch.cos(1.0 + torch.arange(p * 2) * 17.5 * 2 * torch.pi / p)\n",
        "tprint(\"negated close\", torch.allclose(vecp2len[p:], -vecplen, rtol=1e-4, atol=1e-4))\n",
        "tprint(\"vecp2len[p:][:10]\", vecp2len[p:][:10])\n",
        "tprint(\"-vecplen[:10]\", -vecplen[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35db9e86-2b64-4cd3-a4c4-a83d3f88446c",
      "metadata": {
        "scrolled": true,
        "id": "35db9e86-2b64-4cd3-a4c4-a83d3f88446c"
      },
      "outputs": [],
      "source": [
        "def get_c_weights(hook_x, freqs):\n",
        "    DIM_BIAS = torch.mean(hook_x, dim=(-2,-1), keepdim=True)\n",
        "    hook_x2 = interpDFT(hook_x, p*2)\n",
        "    centered_hook_x = torch.zeros_like(hook_x)\n",
        "    for c in range(p):\n",
        "        rolled = torch.roll(hook_x2[...,c,:], 2 * p - c, -1)\n",
        "        centered_hook_x[...,c,:] = rolled[...,::2]\n",
        "    CENTERED_HOOK_X = torch.fft.fft(centered_hook_x)/p\n",
        "\n",
        "    centered_coeffs = CENTERED_HOOK_X[...,[0] + list(freqs)]\n",
        "    waves = torch.view_as_real(centered_coeffs)[...,0].transpose(-2,-1)\n",
        "\n",
        "    weight_waves, bias_wave = torch.cat((waves[...,1:,:], waves[...,1:,:]), -1), 0.5 * waves[...,0,:]\n",
        "    BIAS_COEFFS = torch.fft.fft(bias_wave)[...,[0] + list(freqs)]/p\n",
        "\n",
        "    for f, freq in enumerate(freqs):\n",
        "        if freq % 2 == 1:\n",
        "            weight_waves[...,f,p:] *= -1\n",
        "\n",
        "    WEIGHT_SPECTRUM = torch.fft.fft(weight_waves)/(p*2)\n",
        "    WEIGHT_COEFFS = torch.gather(WEIGHT_SPECTRUM, -1, freqs[None,...,None].expand(WEIGHT_SPECTRUM[...,:1].shape)).squeeze()\n",
        "\n",
        "    return WEIGHT_COEFFS, BIAS_COEFFS, DIM_BIAS\n",
        "\n",
        "def synthesize_hook(weight_coeffs, bias_coeffs, dim_bias, freqs, alpha):\n",
        "    shape = list(weight_coeffs[...,0].shape) + [p,p]\n",
        "    wk = freqs * 2 * torch.pi / p\n",
        "    mags, phases = torch.abs(weight_coeffs).unsqueeze(-2).unsqueeze(-2), torch.angle(weight_coeffs).unsqueeze(-2).unsqueeze(-2)\n",
        "    bias_mags, bias_phases = torch.abs(bias_coeffs).unsqueeze(-2).unsqueeze(-2), torch.angle(bias_coeffs).unsqueeze(-2).unsqueeze(-2)\n",
        "    a, c = tokens[alpha][\"a\"][None,...,None], tokens[alpha][\"c\"][None,...,None]\n",
        "    psquare = a - c/2\n",
        "    centered_waves = torch.cos(psquare * wk)\n",
        "    weight_wave = mags * 4 * torch.cos(phases + c * wk/2)\n",
        "    bias_wave = (bias_mags * 4 * torch.cos(bias_phases + c * torch.cat((torch.zeros_like(wk[0:1]), wk), -1))).sum(-1)\n",
        "    synth = (weight_wave * centered_waves).sum(-1) + bias_wave - dim_bias\n",
        "    return synth\n",
        "\n",
        "hook_a = inputs_last(cache[\"resid_post\"][a_idx,2])\n",
        "hook_c = inputs_last(cache[\"resid_post\"][c_idx,2])\n",
        "full_freqs = key_freqs#torch.cat((key_freqs, key_harmonics), 0)\n",
        "weight_coeffs, bias_coeffs, dim_bias = get_c_weights(hook_c, full_freqs)\n",
        "tprint(\"weight_coeffs\", weight_coeffs[0])\n",
        "synth_hook_a = synthesize_hook(weight_coeffs, bias_coeffs, dim_bias, full_freqs, \"a\")\n",
        "synth_hook_c = synthesize_hook(weight_coeffs, bias_coeffs, dim_bias, full_freqs, \"c\")\n",
        "non_key_freqs = [x for x in range(p//2+1) if x not in key_freqs.tolist()]\n",
        "tprint(\"non_key_freqs\", non_key_freqs)\n",
        "hook_a = zero_out_freqs(hook_a, non_key_freqs, [-2,-1])\n",
        "synth_hook_a = zero_out_freqs(synth_hook_a, non_key_freqs, [-2,-1])\n",
        "showVector([hook_a, synth_hook_a])\n",
        "# hook_c = zero_out_freqs(hook_c, non_key_freqs, [-2])\n",
        "# synth_hook_c = zero_out_freqs(synth_hook_c, non_key_freqs, [-2])\n",
        "showVector([hook_c, synth_hook_c])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e10d81e-b7cd-437b-8219-cd9ca76b2349",
      "metadata": {
        "id": "1e10d81e-b7cd-437b-8219-cd9ca76b2349"
      },
      "outputs": [],
      "source": [
        "# showVector(unembed)\n",
        "# showVector(cache[\"resid_post\"][:,2])\n",
        "# showVector(c_hook(\"resid_post\")[2])\n",
        "unembed = cache[\"unembed.W_U\"]\n",
        "uspec = torch.fft.fft(unembed)\n",
        "allumags = torch.abs(uspec)/p\n",
        "alluphases = torch.angle(uspec)\n",
        "umags = torch.abs(uspec[...,key_freqs])\n",
        "uphases = torch.angle(uspec[...,key_freqs])\n",
        "tprint(\"unembed base phases\", uphases.shape, uphases[:5])\n",
        "\n",
        "def synthesize_resid_post(alpha, freqs, mags, phases):\n",
        "    global wk,bwk,p5,p6,p7,synth\n",
        "    synth = torch.zeros_like(inputs_last(cache[\"resid_post\"]), dtype=torch.float)\n",
        "    a, b, c = expand_all_left(synth, tokens[alpha][\"a\"], tokens[alpha][\"b\"], tokens[alpha][\"c\"])\n",
        "    mags, phases = mags.unsqueeze(-2).expand_as(synth[...,:mags.size(-1)]), phases.unsqueeze(-2).expand_as(synth[...,:phases.size(-1)])\n",
        "    tprint(\"mags\", mags.shape, \"phases\", phases.shape, \"synth\", synth.shape, \"a\", a.shape, \"b\", b.shape, \"c\", c.shape, \"freqs\", freqs.shape)\n",
        "    freqs = expand_all_left(mags, freqs)[0].unsqueeze(-2)\n",
        "    tprint(\"freqs\", freqs.dtype, freqs.shape)\n",
        "\n",
        "    wk = freqs * 2 * torch.pi / p\n",
        "    a, b, c = a.unsqueeze(-1), b.unsqueeze(-1), c.unsqueeze(-1)\n",
        "    tprint(\"wk\", wk.shape, \"\\na\", a.shape, a[-1,0,:5,:5,0], \"\\nb\", b.shape, b[-1,0,:5,:5,0], \"\\nc\", c.shape, c[-1,0,:5,:5,0])\n",
        "\n",
        "    awk, bwk = a * wk, b * wk\n",
        "\n",
        "    phases = phases.unsqueeze(-3)\n",
        "    p5 = torch.exp(1j * (bwk + phases)) * torch.exp(1j * (awk))\n",
        "    mags = mags.unsqueeze(-3)\n",
        "    tprint(\"awk\", awk.shape, \"bwk\", bwk.shape, \"phases\", phases.shape, \"p5\", p5.shape, \"mags\", mags.shape)\n",
        "    p6 = mags * p5\n",
        "    p7 = p6.real\n",
        "\n",
        "    printvars(wk,bwk,p5,p6,p7,synth)\n",
        "    # tprint(\"\n",
        "    # synth[:] += (mags * torch.exp(1j * (b * freqs * 2 * torch.pi / p + phases))).real\n",
        "    synth += p7.sum(-1)\n",
        "    return synth\n",
        "\n",
        "synth = synthesize_resid_post(\"a\", key_freqs, umags[None], uphases[None])\n",
        "showVector(synth[-1])\n",
        "showVector(inputs_last(cache[\"resid_post\"])[2])\n",
        "\n",
        "showVector(inputs_last(inputs_first(synth)[c_idx]))\n",
        "showVector(c_hook(\"resid_post\"))\n",
        "\n",
        "# synth_c = synthesize_resid_post(\"c\", key_freqs, umags[None], uphases[None])\n",
        "# showVector(inputs_last(inputs_first(synth_c)[c_idx])[-1])\n",
        "# showVector(inputs_last(cache[\"resid_post\"][c_idx])[2])\n",
        "\n",
        "for pos in range(p):\n",
        "    wk = torch.exp(1j * -pos * key_freqs * 2 * torch.pi / p)\n",
        "    resid = inputs_last(cache[\"resid_post\"])[2,:,pos]\n",
        "    rspec = torch.fft.fft(resid)\n",
        "    rphases = torch.angle(rspec[...,key_freqs] * wk.unsqueeze(0))\n",
        "\n",
        "    total = 0\n",
        "    diff, absdiff = 0.0, 0.0\n",
        "    for d in range(d_model):\n",
        "        for f, freq in enumerate(key_freqs):\n",
        "            u, r = uphases[d][f].item(), rphases[d][f].item()\n",
        "            r = r + 2 * torch.pi if u > torch.pi/2 and r < -torch.pi/2 else r\n",
        "            u = u + 2 * torch.pi if r > torch.pi/2 and u < -torch.pi/2 else u\n",
        "            absdiff += np.abs(u - r)\n",
        "            diff += (u - r)\n",
        "            total = total + 1\n",
        "    tprint(\"pos\", pos, \"total\", total, \"absdiff\", absdiff, \"phase delta\", absdiff/total, \"diff\", round(diff, 4), \"rspec\", rspec.shape, \"wk\", wk.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6ace6e-e7de-48a2-8dc3-48872c7ff499",
      "metadata": {
        "id": "df6ace6e-e7de-48a2-8dc3-48872c7ff499"
      },
      "outputs": [],
      "source": [
        "pc = 0.9\n",
        "w = 14 * 2 * torch.pi / p\n",
        "# cef = torch.polar(torch.tensor([1.0]), torch.tensor([pc]) - torch.arange(p) * w) * torch.exp(1j * w)\n",
        "# cer = torch.polar(torch.tensor([1.0]), torch.tensor([-pc])) * torch.exp(1j * w)\n",
        "# cep = cef * cer\n",
        "# cf, cr, cp, sf, sr, sp = cef.real, cer.real, cep.real, cef.imag, cer.imag, cep.imag\n",
        "wave_a, wave_b, wave_p, wave_s = [], [], [], []\n",
        "for i in range(p):\n",
        "    wave_a.append((torch.polar(torch.tensor([1.0]), torch.tensor([pc]) - i * w) * torch.exp(1j * torch.arange(p) * w)).real)\n",
        "    wave_b.append((torch.polar(torch.tensor([1.0]), torch.tensor([-pc])) * torch.exp(1j * torch.arange(p) * w)).real)\n",
        "    wave_p.append(wave_a[i] * wave_b[i])\n",
        "    wave_s.append(1.0 * wave_a[i] + 1.0 * wave_b[i])\n",
        "wave_a, wave_b, wave_p, wave_s = torch.stack(wave_a), torch.stack(wave_b), torch.stack(wave_p), torch.stack(wave_s)\n",
        "waves = torch.stack((wave_a, wave_b, wave_p, wave_s))\n",
        "# showVector(waves)\n",
        "x = cache[\"resid_pre\"]\n",
        "# x[...,50:] = 0.0\n",
        "# x[...,0:49] = 0.0\n",
        "# showVector(c_hook(x))\n",
        "W_Q, W_K, W_V = cache[\"W_Q\"], cache[\"W_K\"], cache[\"W_V\"]\n",
        "q = einops.einsum(W_Q, x, \"i d h, b p d -> b p i h\")# [nh C hs] @ [B T C] = [B T nh hs]\n",
        "k = einops.einsum(W_K, x, \"i d h, b p d -> b p i h\")# [nh C hs] @ [B T C] = [B T nh hs]\n",
        "v = einops.einsum(W_V, x, \"i d h, b p d -> b p i h\")# [nh C hs] @ [B T C] = [B T nh hs]\n",
        "q,k,v = cache[\"q\"], cache[\"k\"], cache[\"v\"]\n",
        "# showVector(torch.stack((c_hook(v), c_hook(cache[\"v\"])/30.0)))\n",
        "q = einops.rearrange(q, \"b p i h -> b p i h\")\n",
        "k = einops.rearrange(k, \"b p i h -> b p i h\")\n",
        "v = einops.rearrange(v, \"b p i h -> b p i h\")\n",
        "q = q.transpose(1,2)# [B T nh hs] -> [B nh T hs]\n",
        "k = k.transpose(1,2).transpose(-2, -1)# [B T nh hs] -> [B nh T hs] -> [B nh hs T]\n",
        "v = v.transpose(1,2)\n",
        "\n",
        "attn_scores = torch.matmul(q, k)/np.sqrt(d_head)# [B nh T hs] @ [B nh hs T] = [B nh T T]\n",
        "attn_scores_masked = attn_scores + torch.full([1, 3, 3], -torch.inf).triu(1).to(k.device)# [B nh T T]\n",
        "pattern = F.softmax(attn_scores_masked, -1)# [B nh T T]\n",
        "z = einops.einsum(v, pattern, \"batch mhead k_pos d_head, batch mhead q_pos k_pos -> batch mhead q_pos d_head\")\n",
        "hook_z = einops.rearrange(z, \"batch head q_pos d_head -> batch q_pos head d_head\", head=n_heads)\n",
        "# showVector(c_hook(hook_z))\n",
        "tprint(pattern[:10,0].shape, pattern[c_idx][:10,0])\n",
        "# pattern[...,[0, 1]] = pattern[...,[1, 0]]\n",
        "pattern[...,0] = 0.463\n",
        "pattern[...,1] = 0.537\n",
        "pattern[...,2] = 0.0\n",
        "tprint(pattern[:10,0].shape, pattern[c_idx][:10,0])\n",
        "# tprint(pattern[:10])\n",
        "z = einops.einsum(v, pattern, \"batch mhead k_pos d_head, batch mhead q_pos k_pos -> batch mhead q_pos d_head\")\n",
        "nu_hook_z = einops.rearrange(z, \"batch head q_pos d_head -> batch q_pos head d_head\", head=n_heads)\n",
        "showVector(torch.stack((c_hook(hook_z), c_hook(nu_hook_z)))[:,-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da278e05-45bf-495e-bc56-91b3fb3354da",
      "metadata": {
        "id": "da278e05-45bf-495e-bc56-91b3fb3354da"
      },
      "outputs": [],
      "source": [
        "print(\"weights\\n\")\n",
        "for name, x in cache.items():\n",
        "    if \"hook\" not in name: tprint(name, x.shape)\n",
        "print(\"\\nactivations\\n\")\n",
        "for name, x in cache.items():\n",
        "    if \"hook\" in name: tprint(name, x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ed60bf1-8b1b-4a1a-9f6c-6768e62f19ef",
      "metadata": {
        "id": "5ed60bf1-8b1b-4a1a-9f6c-6768e62f19ef"
      },
      "outputs": [],
      "source": [
        "hook_names_ordered = [\n",
        "    \"hook_embed\",\n",
        "    \"hook_pos_embed\",\n",
        "    \"blocks.0.hook_resid_pre\",\n",
        "    \"blocks.0.attn.hook_q\",\n",
        "    \"blocks.0.attn.hook_k\",\n",
        "    \"blocks.0.attn.hook_v\",\n",
        "    \"blocks.0.attn.hook_attn_scores\",\n",
        "    \"blocks.0.attn.hook_pattern\",\n",
        "    \"blocks.0.attn.hook_z\",\n",
        "    \"blocks.0.attn.hook_result\",\n",
        "    \"blocks.0.hook_attn_out\",\n",
        "    \"blocks.0.hook_resid_mid\",\n",
        "    \"blocks.0.mlp.hook_pre\",\n",
        "    \"blocks.0.mlp.hook_post\",\n",
        "    \"blocks.0.hook_mlp_out\",\n",
        "    \"blocks.0.hook_resid_post\",\n",
        "    \"hook_logits\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf32c6f-2874-4b78-8ef2-84294a718be4",
      "metadata": {
        "id": "caf32c6f-2874-4b78-8ef2-84294a718be4"
      },
      "outputs": [],
      "source": [
        "def get_full_cache_name(name):\n",
        "    found = 0\n",
        "    ret_name = \"\"\n",
        "    for full_name in cache:\n",
        "        if name in full_name:\n",
        "            ret_name = full_name\n",
        "            found = found + 1\n",
        "    if found > 1: print(\"the key given matched more than one full_name in the cache\", name)\n",
        "    return ret_name\n",
        "\n",
        "def get_pos_to_analyze(hook_name):\n",
        "    name = get_full_cache_name(hook_name)\n",
        "    retval = None\n",
        "    if (\"embed\" in name or \"attn.\" in name) and \"hook_z\" not in name:\n",
        "        retval = slice(0,2)\n",
        "    else:\n",
        "        retval = 2\n",
        "    print(hook_name, \"pos return value\", retval)\n",
        "    return retval\n",
        "\n",
        "def get_cpos_to_analyze(hook_name):\n",
        "    name = get_full_cache_name(hook_name)\n",
        "    retval = None\n",
        "    if (\"embed\" in name or \"attn.\" in name) and \"hook_z\" not in name:\n",
        "        retval = 0\n",
        "    else:\n",
        "        retval = slice(0,p)\n",
        "    print(hook_name, \"cpos return value\", retval)\n",
        "    return retval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4baa64be-e563-4188-b263-2e7bc56c909a",
      "metadata": {
        "id": "4baa64be-e563-4188-b263-2e7bc56c909a"
      },
      "outputs": [],
      "source": [
        "for hook_name in hook_names_ordered:\n",
        "    if \"hook\" in hook_name:\n",
        "        sym = get_hook_correlations(c_hook(hook_name), key_harmonics, pos=get_pos_to_analyze(hook_name), cpos=get_cpos_to_analyze(hook_name))\n",
        "        tprint(\"\\n\" + hook_name + \"\\nsymmetries\\n\")\n",
        "        tprint(sym)\n",
        "\n",
        "# hook_name = \"hook_k\"\n",
        "# sym = get_hook_correlations(c_hook(hook_name), key_harmonics, pos=slice(0,2))\n",
        "# tprint(\"\\nsymmetries\\n\")\n",
        "# tprint(sym)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a9f1177-d859-4208-9cbe-9b3b450b09f2",
      "metadata": {
        "id": "8a9f1177-d859-4208-9cbe-9b3b450b09f2"
      },
      "source": [
        "hook_attn_out:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f00dcc-76f1-41bd-a573-533fbffa9c56",
      "metadata": {
        "id": "93f00dcc-76f1-41bd-a573-533fbffa9c56"
      },
      "source": [
        "not a theory: hook_attn_out = sum of (each head score * hook_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67722b50-f1e6-401c-a155-ca4d151173af",
      "metadata": {
        "id": "67722b50-f1e6-401c-a155-ca4d151173af"
      },
      "outputs": [],
      "source": [
        "hook_attn_out = c_hook(\"hook_attn_out\")[:,:2]\n",
        "print(\"hook_attn_out\", hook_attn_out.shape)\n",
        "showVector(pull_out_freqs(hook_attn_out, key_freqs, dims=[-1]).transpose(0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be0eb684-ef96-4edc-b1f3-fbf748e1cf1f",
      "metadata": {
        "scrolled": true,
        "id": "be0eb684-ef96-4edc-b1f3-fbf748e1cf1f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9c10836-0ed1-4fa2-8ad8-c3a3a702d141",
      "metadata": {
        "editable": true,
        "scrolled": true,
        "tags": [],
        "id": "a9c10836-0ed1-4fa2-8ad8-c3a3a702d141"
      },
      "outputs": [],
      "source": [
        "neurons_pre = cache[\"blocks.0.mlp.hook_pre\"]\n",
        "neuron_acts = cache[\"blocks.0.mlp.hook_post\"]\n",
        "# Center the neurons to remove the constant term\n",
        "neuron_acts_centered = neuron_acts - einops.reduce(neuron_acts, 'batch neuron -> 1 neuron', 'mean')\n",
        "# for name, t in cache.items(): print(name, t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ce5c48-377e-4b38-b07b-14b75eef63c8",
      "metadata": {
        "id": "85ce5c48-377e-4b38-b07b-14b75eef63c8"
      },
      "outputs": [],
      "source": [
        "# def extract_freq_2d(tensor, freq):\n",
        "#     # Takes in a pxpx... or batch x ... tensor, returns a 3x3x... tensor of the\n",
        "#     # Linear and quadratic terms of frequency freq\n",
        "#     if tensor.shape[0]==p*p: tensor = einops.rearrange(tensor, '(x y) ... -> x y ...', x=p, y=p)\n",
        "#     # Extracts the linear and quadratic terms corresponding to frequency freq\n",
        "#     index_1d = [0, 2*freq-1, 2*freq]\n",
        "#     # Some dumb manipulation to use fancy array indexing rules\n",
        "#     # Gets the rows and columns in index_1d\n",
        "#     print(\"index_1d\", [[i]*3 for i in index_1d], [index_1d]*3)\n",
        "#     rv = tensor[[[i]*3 for i in index_1d], [index_1d]*3]\n",
        "#     return rv\n",
        "\n",
        "# def get_best_freqs(x):\n",
        "#     x = inputs_last(x)\n",
        "#     print(\"x shhape\", x.shape)\n",
        "#     x = x - einops.reduce(x, '... a b -> ... 1 1', 'mean')\n",
        "#     X = fft2d(inputs_last(x))\n",
        "#     X = X.transpose(0,-1)\n",
        "#     d_X = X.shape[-1]\n",
        "#     freqs, second_freqs, frac_explained, second_frac_explained = [], [], [], []\n",
        "\n",
        "#     for d in range(d_X):\n",
        "#         best_frac_explained = -1e6\n",
        "#         best_freq = -1\n",
        "#         second_best_frac_explained = -1e6\n",
        "#         second_best_freq = -1\n",
        "#         for freq in range(1, p//2):\n",
        "#             # We extract the linear and quadratic fourier terms of frequency freq,\n",
        "#             # and look at how much of the variance of the full vector this explains\n",
        "#             # If neurons specialise into specific frequencies, one frequency should\n",
        "#             # have a large value\n",
        "#             frac = (extract_freq_2d(X[:, :, d], freq).pow(2).sum()/\n",
        "#                               X[:, :, d].pow(2).sum()).item()\n",
        "#             if frac > best_frac_explained:\n",
        "#                 second_best_freq = best_freq\n",
        "#                 second_best_frac_explained = best_frac_explained\n",
        "#                 best_freq = freq\n",
        "#                 best_frac_explained = frac\n",
        "#             elif frac > second_best_frac_explained:\n",
        "#                 second_best_freq = freq\n",
        "#                 second_best_frac_explained = frac\n",
        "#         freqs.append(best_freq)\n",
        "#         frac_explained.append(best_frac_explained)\n",
        "#         second_freqs.append(second_best_freq)\n",
        "#         second_frac_explained.append(second_best_frac_explained)\n",
        "#     freqs = np.array(freqs)\n",
        "#     frac_explained = np.array(frac_explained)\n",
        "#     freqs[frac_explained < 0.25] = -1.\n",
        "\n",
        "#     key_freqs_plus = np.concatenate([key_freqs, np.array([-1])])\n",
        "#     for i in range(len(key_freqs_plus)):\n",
        "#         print(f'Cluster {i}: freq {key_freqs_plus[i]}. {(freqs==key_freqs_plus[i]).sum()} neurons')\n",
        "#     return freqs, frac_explained\n",
        "\n",
        "# freqs_path = \"best_freqs.pt\"\n",
        "# best_freqs = {}\n",
        "# # neuron_acts = cache[\"blocks.0.mlp.hook_post\"]\n",
        "# # # Center the neurons to remove the constant term\n",
        "# # neuron_acts_centered = neuron_acts - einops.reduce(neuron_acts, 'batch neuron -> 1 neuron', 'mean')\n",
        "\n",
        "# if os.path.isfile(freqs_path):\n",
        "#     best_freqs = torch.load(freqs_path)\n",
        "# else:\n",
        "#     for name in cache.keys():\n",
        "#         if \"hook\" in name and cache[name].shape[-1] in [d_mlp, d_model]:\n",
        "#             _, bf, _ = get_freq_indices(cache[name], 5)\n",
        "#             best_freqs[name] = bf\n",
        "#     print(\"best freqs\", [name + str(tstr(b[:5])) for name, b in best_freqs.items()])\n",
        "#     torch.save(best_freqs, freqs_path)\n",
        "\n",
        "#############################################\n",
        "\n",
        "# neuron_freqs, neuron_frac_explained = get_best_freqs(neuron_acts_centered)\n",
        "# neuronsab = neuron_acts.unflatten(0, (p,p))\n",
        "\n",
        "# freq_pairs = {}\n",
        "\n",
        "# for i in range(d_mlp):\n",
        "#     tta, ttav = get_top_k_freqs(neuronsab[..., i], 5, 0, [1])\n",
        "#     freq_pair = (tta[1].item(), tta[2].item(), tta[3].item())\n",
        "#     ratio = 666.0\n",
        "#     if not tta[2].item() in [14,35,41,52]: ratio = (ttav[1]/ttav[2]).item()\n",
        "#     freq_pairs[freq_pair] = freq_pairs.get(freq_pair, [])\n",
        "#     freq_pairs[freq_pair].append(ratio)\n",
        "#     # print(\"dim\", i, \"getting top k freqs for a\", list(neuronsab[..., i].shape), tta[1:].tolist(), torch.round(ttav[1:]).tolist(), \"ratio\", ratio)\n",
        "#     ttb, ttbv = get_top_k_freqs(neuronsab[..., i], 5, 1, [0])\n",
        "#     # print(\"getting top k freqs for b\", list(neuronsab[..., i].shape), ttb.tolist(), torch.round(ttbv, decimals=2).tolist())\n",
        "#     if tta.tolist() != ttb.tolist():\n",
        "#         print(\"not equal freqz\")\n",
        "#         print(\"getting top k freqs for a\", list(neuronsab[..., i].shape), tta[1:].tolist(), torch.round(ttav[1:], decimals=2).tolist())\n",
        "#         print(\"getting top k freqs for b\", list(neuronsab[..., i].shape), ttb[1:].tolist(), torch.round(ttbv[1:], decimals=2).tolist())\n",
        "#         print(\"neuron freq\", neuron_freqs[i], \"frac explained\", neuron_frac_explained[i], \"second freq\", second_neuron_freqs[i], \"second frac explained\", second_neuron_frac_explained[i])\n",
        "\n",
        "# for key in freq_pairs.keys():\n",
        "#     ratios = freq_pairs[key]\n",
        "#     rten = torch.tensor(ratios)\n",
        "#     print(\"\\nfreq pair\", key, \"count\", len(ratios), \"\\nratios\\n\", ratios)\n",
        "#     print(\"min\", rten.min().item(), \"max\", rten.max().item(), \"stddev\", rten.std().item(), \"mean\", rten.mean().item())\n",
        "# key_freqs, neuron_freq_counts = np.unique(neuron_freqs, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7658b99f-5b63-4896-b770-82796e566b00",
      "metadata": {
        "editable": true,
        "scrolled": true,
        "tags": [],
        "id": "7658b99f-5b63-4896-b770-82796e566b00"
      },
      "outputs": [],
      "source": [
        "def get_top_k_quadratic_terms(spec, k):\n",
        "    top = []\n",
        "    lintop = []\n",
        "    for freq in range(1,p//2+1):\n",
        "        box = spec[freq*2:freq*2+2,freq*2:freq*2+2]\n",
        "        mag = box.flatten().abs().max().item()\n",
        "        linbox1 = spec[0:1,freq*2:freq*2+2]\n",
        "        linbox2 = spec[freq*2:freq*2+2,0:1]\n",
        "        linmag = (linbox1.flatten().abs().max() + linbox2.flatten().abs().max()).item() / 2\n",
        "        if len(top) < k:\n",
        "            top.append((freq, round(mag,1)))#, [round(num,1) for num in box.flatten().tolist()]))\n",
        "            if len(top) == k:\n",
        "                top = sorted(top, key=lambda x: x[1])\n",
        "        elif mag > top[0][1]:\n",
        "            top[0] = (freq, round(mag,1))\n",
        "            top = sorted(top, key=lambda x: x[1])\n",
        "        if len(lintop) < k:\n",
        "            lintop.append((freq, round(linmag,1)))#, [round(num,1) for num in box.flatten().tolist()]))\n",
        "            if len(lintop) == k:\n",
        "                lintop = sorted(lintop, key=lambda x: x[1])\n",
        "        elif linmag > lintop[0][1]:\n",
        "            lintop[0] = (freq, round(linmag,1))\n",
        "            lintop = sorted(lintop, key=lambda x: x[1])\n",
        "    top = sorted(top, key=lambda x: x[1], reverse=True)\n",
        "    lintop = sorted(lintop, key=lambda x: x[1], reverse=True)\n",
        "    return top, lintop\n",
        "\n",
        "def get_top_k_linear_terms(x, k):\n",
        "    mags = torch.fft.fft(x, dim=-1).abs().sum(-1)[...,:p//2+1] + torch.fft.fft(x, dim=-2).abs().sum(-2)[...,:p//2+1]\n",
        "    mags[0] = 0\n",
        "    mags, freqs = mags.topk(k)\n",
        "    return sorted(dict(zip(freqs.tolist(), mags.tolist())), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "def get_top_k_sums(x, k):\n",
        "    X = fft2d(x)\n",
        "    topk = {}\n",
        "    lintopk = {}\n",
        "    for d in range(x.size(0)):\n",
        "        top, lintop = get_top_k_quadratic_terms(X[d], k)\n",
        "        # print(\"lintop linear 2d\", len(lintop), lintop)\n",
        "        # lintop = get_top_k_linear_terms(x[d], k)\n",
        "        # print(\"fft linear\", len(lintop.items()), lintop)\n",
        "        if d == 0: print(top)\n",
        "        for i in range(len(top)):\n",
        "            freq = top[i][0]\n",
        "            if freq in topk:\n",
        "                topk[freq] += top[i][1]\n",
        "            else:\n",
        "                topk[freq] = top[i][1]\n",
        "            freq = lintop[i][0]\n",
        "            if freq in lintopk:\n",
        "                lintopk[freq] += lintop[i][1]\n",
        "            else:\n",
        "                lintopk[freq] = lintop[i][1]\n",
        "    topk = dict(sorted(topk.items(), key=lambda item: item[1], reverse=True))\n",
        "    lintopk = dict(sorted(lintopk.items(), key=lambda item: item[1], reverse=True))\n",
        "    return topk, lintopk\n",
        "\n",
        "def inspect_quad(spec):\n",
        "    if spec.size(0) == p*p: spec = spec.unflatten(0,(p,p))\n",
        "    def describe(idx): return \"Top 3 Freqs: \" + str(get_top_k_quadratic_terms(spec[idx], 3))\n",
        "    get_label = lambda x,start,end : str(x) if x==0 else \"sin \" + str(x//2+1) if x%2==1 else \"cos \" + str(x//2)\n",
        "    lp.draw_matrix(spec.pow(2).sqrt(), xmap=get_label, ymap=get_label, descriptor=describe)\n",
        "\n",
        "def inspect_quads(specs, title = \"Tensors\", names = []):\n",
        "    N = len(specs)\n",
        "\n",
        "    if names == []: names = list(map(chr, range(ord('A'), ord('A')+N)))\n",
        "\n",
        "    print(\"specs[0].shape\", specs[0].shape, specs[1].shape)\n",
        "\n",
        "    spec = torch.empty(0).to(specs[0].device)\n",
        "\n",
        "    for i in range(len(specs)):\n",
        "        if specs[i].size(0) == p*p: specs[i] = specs[i].unflatten(0,(p,p))\n",
        "        if specs[i].size(-1) == p*p: specs[i] = specs[i].unflatten(-1,(p,p))\n",
        "        if specs[i].size(-1) % p != 0: specs[i] = specs[i].permute(-1,0,1)\n",
        "        spec = torch.cat((spec, specs[i].unsqueeze(1)), 1)\n",
        "\n",
        "    dim_len = specs[0].size(0)\n",
        "    spec = spec.flatten(0,1)\n",
        "\n",
        "    print(\"specs[0].shape\", specs[0].shape, spec.shape)\n",
        "\n",
        "    def describe(idx):\n",
        "        n=idx%N\n",
        "        return str(idx//N) + names[n] + \" \" + title + \": \" + str(get_top_k_quadratic_terms(spec[idx], 5))\n",
        "    get_label = lambda x,start,end : str(x) if x==0 else \"sin \" + str(x//2+1) if x%2==1 else \"cos \" + str(x//2)\n",
        "    lp.draw_matrix(spec.pow(2).sqrt(), xmap=get_label, ymap=get_label, descriptor=describe)\n",
        "\n",
        "# inspect_quad(fft2d(neuron_acts.unflatten(0,(p,p)).permute(-1,0,1)))\n",
        "# inspect_quad(fft2d(neurons_pre.unflatten(0,(p,p)).permute(-1,0,1)))\n",
        "\n",
        "def compare_spectrums(before, after, title = \"\"):\n",
        "    pre = inputs_last(before)\n",
        "    post = inputs_last(after)\n",
        "    print(\"pre\", pre.shape, \"sum\", round(pre.abs().sum().item(), 2), \"post\", post.shape, \"sum\", round(post.abs().sum().item(), 2))\n",
        "    top_pre = get_top_k_sums(pre, 10)\n",
        "    top_post = get_top_k_sums(post, 10)\n",
        "    combined = [{}, {}]\n",
        "    for i in range(2):\n",
        "        for key, value in top_pre[i].items():\n",
        "            combined[i][key] = (round(value,1), 0.0)\n",
        "        for key, value in top_post[i].items():\n",
        "            if key in combined[i]:\n",
        "                combined[i][key] = (combined[i][key][0], round(value,1))\n",
        "            else:\n",
        "                combined[i][key] = (0.0, round(value,1))\n",
        "    print(title)\n",
        "    for freq, mags in combined[0].items():\n",
        "        print(\"quadratic freq\", freq, \"before\", mags[0], \"after\", mags[1])\n",
        "    for freq, mags in combined[1].items():\n",
        "        print(\"linear freq\", freq, \"before\", mags[0], \"after\", mags[1])\n",
        "\n",
        "\n",
        "pre_attn = cache[\"blocks.0.hook_resid_pre\"]\n",
        "post_attn = cache[\"blocks.0.hook_attn_out\"]\n",
        "pre_pos_embed = cache[\"hook_embed\"][:,0,:]\n",
        "post_pos_embed = cache[\"hook_pos_embed\"][:,0,:]\n",
        "\n",
        "\n",
        "# inspect_quads([fft2d(inputs_last(neurons_pre)), fft2d(inputs_last(neuron_acts))], \"Neurons\")\n",
        "# inspect_quads([fft2d(inputs_last(pre_attn * 500000)), fft2d(inputs_last(post_attn))], \"Attention\")\n",
        "\n",
        "# compare_spectrums(neurons_pre, neuron_acts)\n",
        "# compare_spectrums(pre_attn * 500000, post_attn, \"Pre/Post Attn\")\n",
        "compare_spectrums(pre_pos_embed * 500000, post_pos_embed * 500000, \"Pre/Post Pos Embed\")\n",
        "linears = [46, 49, 32, 56, 39, 48, 41, 52, 25, 43]\n",
        "sums = {}\n",
        "diffs = {}\n",
        "for l in linears:\n",
        "    for m in linears:\n",
        "        sum = l + m\n",
        "        if sum > p//2:\n",
        "            sum = round(p/2 - (sum - p/2))\n",
        "        diff = round(abs(l - m))\n",
        "        sums[sum] = sums[sum] + 1 if sum in sums else 1\n",
        "        diffs[diff] = diffs[diff] + 1 if diff in diffs else 1\n",
        "\n",
        "print(\"sums\", sorted(sums.items(), key=lambda x : x[1], reverse=True))\n",
        "print(\"diffs\", sorted(diffs.items(), key=lambda x : x[1], reverse=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}